#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import csv
import os
import json
import uuid
import logging
import time
import random
from urllib.parse import urljoin, urlparse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("betaren_fixed.log", encoding='utf-8', mode='w'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("BetarenFixed")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ .env
OUTPUT_DIR = os.getenv('DOWNLOAD_DIR', 'downloads')
CHROMEDRIVER_PATH = os.getenv('CHROMEDRIVER_PATH', 'D:/crawler_risks/chromedriver.exe')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
SLEEP_BETWEEN_REQUESTS = float(os.getenv('SLEEP_BETWEEN_REQUESTS', '2.0'))

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
BASE_URL = "https://betaren.ru"
IMAGES_DIR = os.path.join(OUTPUT_DIR, 'images')

# User-Agent —Å–ø–∏—Å–æ–∫
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.6943.142 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15'
]


def create_directories():
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ–π"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(IMAGES_DIR, exist_ok=True)

    for folder in ["diseases", "pests", "weeds"]:
        os.makedirs(os.path.join(IMAGES_DIR, folder), exist_ok=True)

    logger.info("‚úÖ –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó —Å—Ç–≤–æ—Ä–µ–Ω–æ")


def get_webdriver():
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–±-–¥—Ä–∞–π–≤–µ—Ä–∞"""
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument(f'--user-agent={random.choice(USER_AGENTS)}')
    options.add_argument('--disable-blink-features=AutomationControlled')

    service = Service(CHROMEDRIVER_PATH)
    return webdriver.Chrome(service=service, options=options)


def translate_text_gpt(text, target_language="ukrainian"):
    """–ü–µ—Ä–µ–∫–ª–∞–¥ —Ç–µ–∫—Å—Ç—É —á–µ—Ä–µ–∑ OpenAI GPT API"""
    if not text or not text.strip() or not OPENAI_API_KEY:
        return ""

    try:
        url = "https://api.openai.com/v1/chat/completions"

        headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }

        if target_language == "ukrainian":
            lang_instruction = "–Ω–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–≤—É"
        else:
            lang_instruction = "to English"

        prompt = f"–ü–µ—Ä–µ–≤–µ–¥–∏ —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç {lang_instruction}, —Å–æ—Ö—Ä–∞–Ω—è—è –Ω–∞—É—á–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é:\n\n{text}"

        data = {
            "model": "gpt-3.5-turbo",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 2000,
            "temperature": 0.2
        }

        response = requests.post(url, headers=headers, json=data, timeout=30)
        response.raise_for_status()

        result = response.json()
        translated_text = result['choices'][0]['message']['content'].strip()

        logger.info(f"‚úÖ –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ –Ω–∞ {target_language}: {len(text)} -> {len(translated_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        return translated_text

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
        return ""


def download_image(image_url, filepath, referer=None):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
            'Referer': referer or BASE_URL
        }

        response = requests.get(image_url, headers=headers, timeout=15, stream=True)
        response.raise_for_status()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º content-type
        content_type = response.headers.get('content-type', '').lower()
        if not any(img_type in content_type for img_type in ['image/', 'jpeg', 'jpg', 'png', 'gif', 'webp']):
            logger.warning(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø: {content_type}")
            return False

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
        if os.path.getsize(filepath) < 1024:
            logger.warning(f"‚ùå –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª: {filepath}")
            os.remove(filepath)
            return False

        logger.info(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–∫–∞—á–∞–Ω–æ: {os.path.basename(filepath)}")
        return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {image_url}: {e}")
        return False


def extract_images_from_page(page_url):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    driver = get_webdriver()

    try:
        logger.info(f"üîç –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞: {page_url}")
        driver.get(page_url)

        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        image_urls = set()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º set –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –û—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        main_img = soup.select_one('div.harmful-detail__picture img.gallery__img')
        if main_img and main_img.get('src'):
            img_url = urljoin(page_url, main_img['src'])
            if not any(exclude in img_url.lower() for exclude in ['logo', 'icon', 'banner']):
                image_urls.add(img_url)
                logger.info(f"üì∏ –ù–∞–π–¥–µ–Ω–æ –æ—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –ì–∞–ª–µ—Ä–µ—è
        gallery_imgs = soup.select('div.harmful-detail__picture-gallery img.gallery__img')
        for img in gallery_imgs:
            if img.get('src'):
                img_url = urljoin(page_url, img['src'])
                if not any(exclude in img_url.lower() for exclude in ['logo', 'icon', 'banner']):
                    image_urls.add(img_url)

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –°—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ–ª–Ω–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        gallery_links = soup.select('div.harmful-detail__picture-gallery a.gallery__link')
        for link in gallery_links:
            href = link.get('href', '')
            if any(ext in href.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                img_url = urljoin(page_url, href)
                image_urls.add(img_url)

        result = list(image_urls)
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(result)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        return result

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
        return []
    finally:
        driver.quit()


def get_category_links(category_path):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    driver = get_webdriver()

    try:
        full_url = f"{BASE_URL}{category_path}"
        logger.info(f"üîó –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑: {full_url}")

        driver.get(full_url)
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # –ò—â–µ–º —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (–Ω–µ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ)
        links = []

        # –ú–µ—Ç–æ–¥ 1: –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Å–ø–∏—Å–∫–µ
        for link_elem in soup.find_all('a', href=True):
            href = link_elem['href']
            text = link_elem.get_text(strip=True)

            # –§–∏–ª—å—Ç—Ä—ã –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
            if (href.startswith('/harmful/') and
                    len(text) > 3 and
                    not any(exclude in href.lower() for exclude in [
                        'javascript:', 'mailto:', 'tel:', '#',
                        '/harmful/bolezni/', '/harmful/vrediteli/', '/harmful/sornyaki/',  # –∏—Å–∫–ª—é—á–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                        'sitemap', 'search', 'login', 'register'
                    ]) and
                    # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                    not any(exclude in text.lower() for exclude in [
                        '–∑–∞–∫–∞–∑–∞—Ç—å', 'order', '–≥–ª–∞–≤–Ω–∞—è', 'home', '–Ω–∞–∑–∞–¥', 'back',
                        '–º–µ–Ω—é', 'menu', '–ø–æ–∏—Å–∫', 'search', '–≤–æ–π—Ç–∏', 'login'
                    ]) and
                    href != category_path):  # –Ω–µ —Å–∞–º–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è

                full_item_url = urljoin(BASE_URL, href)
                links.append({
                    'name': text,
                    'url': full_item_url
                })

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        seen_urls = set()
        unique_links = []
        for link in links:
            if link['url'] not in seen_urls:
                seen_urls.add(link['url'])
                unique_links.append(link)

        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(unique_links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")
        return unique_links

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫: {e}")
        return []
    finally:
        driver.quit()


def scrape_detail_page(item_url, category_type):
    """–°–±–æ—Ä –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    driver = get_webdriver()

    try:
        logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {item_url}")
        driver.get(item_url)

        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
        title_elem = soup.find('h1')
        title = title_elem.get_text(strip=True) if title_elem else ""

        if not title or len(title) < 3:
            logger.warning(f"‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ: '{title}'")
            return None

        # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        content_blocks = soup.find_all(['div', 'section'], class_=[
            'harmful-detail__content', 'content', 'description', 'text-content'
        ])

        all_text = ""
        for block in content_blocks:
            text = block.get_text(separator=' ', strip=True)
            if text and len(text) > 50:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –±–ª–æ–∫–∏
                all_text += text + " "

        # –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –±–µ—Ä–µ–º –∏–∑ body
        if not all_text:
            main_content = soup.find('main') or soup.find('article') or soup.find('body')
            if main_content:
                all_text = main_content.get_text(separator=' ', strip=True)

        # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        description_ru = ""
        symptoms_ru = ""
        development_conditions_ru = ""
        control_measures_ru = ""

        # –ü—Ä–∏–º–∏—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        text_parts = all_text.split('.')
        current_section = "description"

        for part in text_parts:
            part = part.strip()
            if len(part) < 10:
                continue

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫ –∫–∞–∫–æ–π —Å–µ–∫—Ü–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Ç–µ–∫—Å—Ç
            part_lower = part.lower()

            if any(keyword in part_lower for keyword in ['—Å–∏–º–ø—Ç–æ–º', '–ø—Ä–∏–∑–Ω–∞–∫', '–ø—Ä–æ—è–≤–ª–µ–Ω', '–ø–æ–≤—Ä–µ–∂–¥–µ–Ω']):
                current_section = "symptoms"
            elif any(keyword in part_lower for keyword in ['—É—Å–ª–æ–≤–∏—è', '—Ä–∞–∑–≤–∏—Ç–∏', '—Ñ–∞–∫—Ç–æ—Ä—ã', '–±–∏–æ–ª–æ–≥']):
                current_section = "development"
            elif any(keyword in part_lower for keyword in ['–∑–∞—â–∏—Ç–∞', '–±–æ—Ä—å–±–∞', '–º–µ—Ä—ã', '–∫–æ–Ω—Ç—Ä–æ–ª', '–ø—Ä–µ–ø–∞—Ä–∞—Ç']):
                current_section = "control"

            # –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π —Å–µ–∫—Ü–∏–∏
            if current_section == "description":
                description_ru += part + ". "
            elif current_section == "symptoms":
                symptoms_ru += part + ". "
            elif current_section == "development":
                development_conditions_ru += part + ". "
            elif current_section == "control":
                control_measures_ru += part + ". "

        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ–ø–∏—Å–∞–Ω–∏–µ
        if not any([description_ru, symptoms_ru, development_conditions_ru, control_measures_ru]):
            description_ru = all_text[:1000] if all_text else ""  # –ü–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤

        # –ü–µ—Ä–µ–≤–æ–¥—ã
        logger.info("üåç –í—ã–ø–æ–ª–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã...")

        description_ua = translate_text_gpt(description_ru, "ukrainian") if description_ru else ""
        description_en = translate_text_gpt(description_ru, "english") if description_ru else ""

        symptoms_ua = translate_text_gpt(symptoms_ru, "ukrainian") if symptoms_ru else ""
        symptoms_en = translate_text_gpt(symptoms_ru, "english") if symptoms_ru else ""

        development_conditions_ua = translate_text_gpt(development_conditions_ru,
                                                       "ukrainian") if development_conditions_ru else ""
        development_conditions_en = translate_text_gpt(development_conditions_ru,
                                                       "english") if development_conditions_ru else ""

        control_measures_ua = translate_text_gpt(control_measures_ru, "ukrainian") if control_measures_ru else ""
        control_measures_en = translate_text_gpt(control_measures_ru, "english") if control_measures_ru else ""

        name_en = translate_text_gpt(title, "english") if title else ""

        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É API –≤—ã–∑–æ–≤–∞–º–∏
        time.sleep(1)

        # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        logger.info("üì∏ –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
        image_urls = extract_images_from_page(item_url)

        item_id = str(uuid.uuid4())
        downloaded_images = []

        if image_urls:
            for i, image_url in enumerate(image_urls):
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
                parsed_url = urlparse(image_url)
                file_extension = os.path.splitext(parsed_url.path)[1] or '.jpg'

                # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Ñ–∞–π–ª–∞
                clean_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).strip()
                clean_title = clean_title.replace(' ', '_')[:30]  # –£–∫–æ—Ä–∞—á–∏–≤–∞–µ–º

                if len(image_urls) == 1:
                    filename = f"{category_type}_{clean_title}_{item_id[:8]}{file_extension}"
                else:
                    filename = f"{category_type}_{clean_title}_{item_id[:8]}_{i + 1:02d}{file_extension}"

                filepath = os.path.join(IMAGES_DIR, category_type, filename)

                # –°–∫–∞—á–∏–≤–∞–µ–º
                if download_image(image_url, filepath, item_url):
                    downloaded_images.append({
                        'image_url': image_url,
                        'image_path': filepath,
                        'filename': filename
                    })

                time.sleep(random.uniform(0.5, 1.0))

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫—É–ª—å—Ç—É—Ä—ã (–±–∞–∑–æ–≤—ã–π —Å–ø–∏—Å–æ–∫)
        crops = []
        full_text_lower = all_text.lower()
        common_crops = ['–ø—à–µ–Ω–∏—Ü–∞', '—Ä–æ–∂—å', '—è—á–º–µ–Ω—å', '–æ–≤–µ—Å', '–∫—É–∫—É—Ä—É–∑–∞', '–ø–æ–¥—Å–æ–ª–Ω–µ—á–Ω–∏–∫', '—Å–æ—è', '—Ä–∞–ø—Å']

        for crop in common_crops:
            if crop in full_text_lower:
                crops.append(crop)

        if not crops:  # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –¥–æ–±–∞–≤–ª—è–µ–º –ø—à–µ–Ω–∏—Ü—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            crops = ['–ø—à–µ–Ω–∏—Ü–∞']

        result = {
            'id': item_id,
            'name': title,
            'name_en': name_en,
            'scientific_name': "",  # –ú–æ–∂–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å
            'description_ru': description_ru.strip(),
            'description_ua': description_ua.strip(),
            'description_en': description_en.strip(),
            'symptoms_ru': symptoms_ru.strip(),
            'symptoms_ua': symptoms_ua.strip(),
            'symptoms_en': symptoms_en.strip(),
            'development_conditions_ru': development_conditions_ru.strip(),
            'development_conditions_ua': development_conditions_ua.strip(),
            'development_conditions_en': development_conditions_en.strip(),
            'control_measures_ru': control_measures_ru.strip(),
            'control_measures_ua': control_measures_ua.strip(),
            'control_measures_en': control_measures_en.strip(),
            'source_urls': item_url,
            'crops': crops,
            'images': downloaded_images,
            'is_active': True,
            'version': 1
        }

        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {title}")
        return result

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {item_url}: {e}")
        return None
    finally:
        driver.quit()


def save_data_to_csv(data, category_type):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ CSV —Ñ–∞–π–ª—ã —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ö–µ–º–µ –ë–î"""
    if not data:
        logger.warning(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ {category_type}")
        return

    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º {len(data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è {category_type}")

    # –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∫ –Ω–∞–∑–≤–∞–Ω–∏—è–º —Ç–∞–±–ª–∏—Ü –ë–î
    table_mapping = {
        'diseases': 'disease',
        'pests': 'vermin',
        'weeds': 'weed'
    }

    table_name = table_mapping.get(category_type, category_type[:-1])

    # –§–∞–π–ª—ã CSV
    main_csv = os.path.join(OUTPUT_DIR, f'{category_type}.csv')
    descriptions_csv = os.path.join(OUTPUT_DIR, f'{table_name}_descriptions.csv')
    images_csv = os.path.join(OUTPUT_DIR, f'{table_name}_images.csv')
    crops_csv = os.path.join(OUTPUT_DIR, f'{table_name}_crops.csv')

    # 1. –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    with open(main_csv, 'w', encoding='utf-8', newline='') as f:
        fieldnames = ['id', 'name', 'name_en', 'scientific_name', 'is_active']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for item in data:
            writer.writerow({
                'id': item['id'],
                'name': item['name'],
                'name_en': item['name_en'],
                'scientific_name': item['scientific_name'],
                'is_active': item['is_active']
            })

    # 2. –¢–∞–±–ª–∏—Ü–∞ –æ–ø–∏—Å–∞–Ω–∏–π (—Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ö–µ–º–µ –ë–î)
    with open(descriptions_csv, 'w', encoding='utf-8', newline='') as f:
        if category_type == 'diseases':
            fieldnames = [
                'id', 'disease_id', 'description_ru', 'description_ua', 'description_en',
                'symptoms_ru', 'symptoms_ua', 'symptoms_en',
                'development_conditions_ru', 'development_conditions_ua', 'development_conditions_en',
                'control_measures_ru', 'control_measures_ua', 'control_measures_en',
                'photo_path', 'version'
            ]
        elif category_type == 'pests':
            fieldnames = [
                'id', 'vermin_id', 'description_ru', 'description_ua', 'description_en',
                'damage_symptoms_ru', 'damage_symptoms_ua', 'damage_symptoms_en',
                'biology_ru', 'biology_ua', 'biology_en',
                'control_measures_ru', 'control_measures_ua', 'control_measures_en',
                'photo_path', 'version'
            ]
        else:  # weeds
            fieldnames = [
                'id', 'weed_id', 'description_ru', 'description_ua', 'description_en',
                'biological_features_ru', 'biological_features_ua', 'biological_features_en',
                'harmfulness_ru', 'harmfulness_ua', 'harmfulness_en',
                'control_measures_ru', 'control_measures_ua', 'control_measures_en',
                'photo_path', 'version'
            ]

        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for item in data:
            # –ü—É—Ç—å –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Ñ–æ—Ç–æ
            main_photo_path = item['images'][0]['image_path'] if item['images'] else ""

            base_row = {
                'id': str(uuid.uuid4()),
                f'{table_name}_id': item['id'],
                'description_ru': item['description_ru'],
                'description_ua': item['description_ua'],
                'description_en': item['description_en'],
                'control_measures_ru': item['control_measures_ru'],
                'control_measures_ua': item['control_measures_ua'],
                'control_measures_en': item['control_measures_en'],
                'photo_path': main_photo_path,
                'version': item['version']
            }

            if category_type == 'diseases':
                base_row.update({
                    'symptoms_ru': item['symptoms_ru'],
                    'symptoms_ua': item['symptoms_ua'],
                    'symptoms_en': item['symptoms_en'],
                    'development_conditions_ru': item['development_conditions_ru'],
                    'development_conditions_ua': item['development_conditions_ua'],
                    'development_conditions_en': item['development_conditions_en']
                })
            elif category_type == 'pests':
                base_row.update({
                    'damage_symptoms_ru': item['symptoms_ru'],
                    'damage_symptoms_ua': item['symptoms_ua'],
                    'damage_symptoms_en': item['symptoms_en'],
                    'biology_ru': item['development_conditions_ru'],
                    'biology_ua': item['development_conditions_ua'],
                    'biology_en': item['development_conditions_en']
                })
            else:  # weeds
                base_row.update({
                    'biological_features_ru': item['symptoms_ru'],
                    'biological_features_ua': item['symptoms_ua'],
                    'biological_features_en': item['symptoms_en'],
                    'harmfulness_ru': item['development_conditions_ru'],
                    'harmfulness_ua': item['development_conditions_ua'],
                    'harmfulness_en': item['development_conditions_en']
                })

            writer.writerow(base_row)

    # 3. –¢–∞–±–ª–∏—Ü–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    with open(images_csv, 'w', encoding='utf-8', newline='') as f:
        fieldnames = ['id', f'{table_name}_id', 'image_path', 'image_url', 'version']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for item in data:
            for image in item['images']:
                writer.writerow({
                    'id': str(uuid.uuid4()),
                    f'{table_name}_id': item['id'],
                    'image_path': image['image_path'],
                    'image_url': image['image_url'],
                    'version': 1
                })

    # 4. –¢–∞–±–ª–∏—Ü–∞ –∫—É–ª—å—Ç—É—Ä
    with open(crops_csv, 'w', encoding='utf-8', newline='') as f:
        fieldnames = [f'{table_name}_id', 'crops']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for item in data:
            for crop in item['crops']:
                writer.writerow({
                    f'{table_name}_id': item['id'],
                    'crops': crop
                })

    logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 4 CSV —Ñ–∞–π–ª–∞ –¥–ª—è {category_type}")


def process_category(category_type, max_items=None):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_type}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    category_paths = {
        'diseases': [
            '/harmful/bolezni/bolezni-zernovykh-kultur/',
            '/harmful/bolezni/bolezni-sadovyh-kultur/',
            '/harmful/bolezni/bolezni-ovoshchnyh-kultur/'
        ],
        'pests': [
            '/harmful/vrediteli/vrediteli-zernovykh-kultur/',
            '/harmful/vrediteli/vrediteli-sadovyh-kultur/',
            '/harmful/vrediteli/vrediteli-ovoshchnyh-kultur/'
        ],
        'weeds': [
            '/harmful/sornyaki/'
        ]
    }

    # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ –≤—Å–µ—Ö –ø–æ–¥–∫ategor–∏–π
    all_links = []
    paths = category_paths.get(category_type, [])

    for path in paths:
        logger.info(f"üìÇ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é: {path}")
        links = get_category_links(path)
        all_links.extend(links)
        time.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏

    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    seen_urls = set()
    unique_links = []
    for link in all_links:
        if link['url'] not in seen_urls:
            seen_urls.add(link['url'])
            unique_links.append(link)

    logger.info(f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(unique_links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è {category_type}")

    if max_items and len(unique_links) > max_items:
        unique_links = unique_links[:max_items]
        logger.info(f"‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ {max_items} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Å—ã–ª–∫—É
    processed_data = []

    for i, link in enumerate(unique_links, 1):
        logger.info(f"üìÑ [{i}/{len(unique_links)}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {link['name']}")

        try:
            detail_data = scrape_detail_page(link['url'], category_type)

            if detail_data:
                processed_data.append(detail_data)
                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {detail_data['name']}")

                # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                if len(processed_data) % 5 == 0:
                    logger.info(f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {len(processed_data)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                    save_data_to_csv(processed_data, category_type)
            else:
                logger.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {link['name']}")

            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(SLEEP_BETWEEN_REQUESTS)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {link['name']}: {e}")
            continue

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    if processed_data:
        save_data_to_csv(processed_data, category_type)
        logger.info(f"üéâ –ö–∞—Ç–µ–≥–æ—Ä–∏—è {category_type} –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed_data)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    else:
        logger.warning(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category_type}")

    return processed_data


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üîß –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô —Å–∫—Ä–∞–ø–µ—Ä Betaren.ru")
    print("   ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫")
    print("   ‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    print("   ‚úÖ –†–∞–±–æ—á–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã —á–µ—Ä–µ–∑ GPT")
    print("   ‚úÖ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print("   ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å—Ö–µ–º–∞ –ë–î")
    print()
    print("1. –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –í–°–ï –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–±–æ–ª–µ–∑–Ω–∏ + –≤—Ä–µ–¥–∏—Ç–µ–ª–∏ + —Å–æ—Ä–Ω—è–∫–∏)")
    print("2. –¢–æ–ª—å–∫–æ –±–æ–ª–µ–∑–Ω–∏")
    print("3. –¢–æ–ª—å–∫–æ –≤—Ä–µ–¥–∏—Ç–µ–ª–∏")
    print("4. –¢–æ–ª—å–∫–æ —Å–æ—Ä–Ω—è–∫–∏")
    print("5. –¢–ï–°–¢: –ø–æ 3 —ç–ª–µ–º–µ–Ω—Ç–∞ –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")

    choice = input("\n–í—ã–±–µ—Ä–∏—Ç–µ –æ–ø—Ü–∏—é (1-5): ").strip()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    if not OPENAI_API_KEY:
        logger.warning("‚ö†Ô∏è OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω - –ø–µ—Ä–µ–≤–æ–¥—ã –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã")
        proceed = input("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –±–µ–∑ –ø–µ—Ä–µ–≤–æ–¥–æ–≤? (y/n): ").strip().lower()
        if proceed != 'y':
            return

    create_directories()

    if choice == "1":
        logger.info("üöÄ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
        process_category('diseases')
        process_category('pests')
        process_category('weeds')

    elif choice == "2":
        logger.info("ü¶† –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –±–æ–ª–µ–∑–Ω–∏")
        process_category('diseases')

    elif choice == "3":
        logger.info("üêõ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –≤—Ä–µ–¥–∏—Ç–µ–ª–µ–π")
        process_category('pests')

    elif choice == "4":
        logger.info("üå± –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ—Ä–Ω—è–∫–∏")
        process_category('weeds')

    elif choice == "5":
        logger.info("üß™ –¢–ï–°–¢–û–í–´–ô —Ä–µ–∂–∏–º: –ø–æ 3 —ç–ª–µ–º–µ–Ω—Ç–∞")
        process_category('diseases', max_items=3)
        process_category('pests', max_items=3)
        process_category('weeds', max_items=3)

    else:
        print("‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≤—ã–±–æ—Ä")
        return

    print("\nüéâ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫–µ: {OUTPUT_DIR}")
    print(f"üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–∞–ø–∫–µ: {IMAGES_DIR}")
    print("üìä CSV —Ñ–∞–π–ª—ã –≥–æ—Ç–æ–≤—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ë–î!")


if __name__ == "__main__":
    main()