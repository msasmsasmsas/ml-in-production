#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import csv
import os
import json
import uuid
import logging
import time
import random
from urllib.parse import urljoin, urlparse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("betaren_universal.log", encoding='utf-8', mode='w'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("BetarenUniversal")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
OUTPUT_DIR = os.getenv('DOWNLOAD_DIR', 'downloads')
CHROMEDRIVER_PATH = os.getenv('CHROMEDRIVER_PATH', 'chromedriver.exe')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
SLEEP_BETWEEN_REQUESTS = float(os.getenv('SLEEP_BETWEEN_REQUESTS', '2.0'))

BASE_URL = "https://betaren.ru"
IMAGES_DIR = os.path.join(OUTPUT_DIR, 'images')

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]


def create_directories():
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(IMAGES_DIR, exist_ok=True)
    for folder in ["diseases", "pests", "weeds"]:
        os.makedirs(os.path.join(IMAGES_DIR, folder), exist_ok=True)
    logger.info("‚úÖ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ–∑–¥–∞–Ω—ã")


def get_webdriver():
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–±-–¥—Ä–∞–π–≤–µ—Ä–∞"""
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument(f'--user-agent={random.choice(USER_AGENTS)}')
    options.add_argument('--disable-blink-features=AutomationControlled')

    service = Service(CHROMEDRIVER_PATH)
    driver = webdriver.Chrome(service=service, options=options)
    return driver


def translate_text_gpt(text, target_language="ukrainian"):
    """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ OpenAI GPT API"""
    if not text or not text.strip() or not OPENAI_API_KEY:
        return ""

    try:
        url = "https://api.openai.com/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }

        if target_language == "ukrainian":
            prompt = f"–ü–µ—Ä–µ–≤–µ–¥–∏ —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π —è–∑—ã–∫, —Å–æ—Ö—Ä–∞–Ω—è—è –Ω–∞—É—á–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é:\n\n{text}"
        else:
            prompt = f"Translate this text to English, preserving scientific terminology:\n\n{text}"

        data = {
            "model": "gpt-3.5-turbo",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 2000,
            "temperature": 0.2
        }

        response = requests.post(url, headers=headers, json=data, timeout=30)
        response.raise_for_status()

        result = response.json()
        translated_text = result['choices'][0]['message']['content'].strip()

        logger.info(f"‚úÖ –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ –Ω–∞ {target_language}")
        return translated_text

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
        return ""


def download_image(image_url, filepath, referer=None):
    """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9',
            'Referer': referer or BASE_URL
        }

        response = requests.get(image_url, headers=headers, timeout=15, stream=True)
        response.raise_for_status()

        content_type = response.headers.get('content-type', '').lower()
        if not any(img_type in content_type for img_type in ['image/', 'jpeg', 'jpg', 'png', 'gif', 'webp']):
            logger.warning(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø: {content_type}")
            return False

        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

        if os.path.getsize(filepath) < 1024:
            logger.warning(f"‚ùå –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Ñ–∞–π–ª")
            if os.path.exists(filepath):
                os.remove(filepath)
            return False

        logger.info(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–∫–∞—á–∞–Ω–æ: {os.path.basename(filepath)}")
        return True

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {image_url}: {e}")
        return False


def extract_images_from_page(soup, page_url):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –†–ï–ê–õ–¨–ù–û–ô —Å—Ç—Ä—É–∫—Ç—É—Ä—ã HTML"""
    try:
        image_urls = []

        # 1. –û–°–ù–û–í–ù–û–ï –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï –∏–∑ div.harmful-detail__picture img
        main_img = soup.select_one('div.harmful-detail__picture img')
        if main_img and main_img.get('src'):
            img_url = urljoin(page_url, main_img['src'])
            image_urls.append(img_url)
            logger.info(f"üì∏ –ù–∞–π–¥–µ–Ω–æ –æ—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")

        # 2. –ì–ê–õ–ï–†–ï–Ø –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô –∏–∑ swiper-slide
        gallery_imgs = soup.select('div.swiper-slide img.gallery__img')
        for img in gallery_imgs:
            if img.get('src'):
                img_url = urljoin(page_url, img['src'])
                if img_url not in image_urls:
                    image_urls.append(img_url)

        # 3. –°–°–´–õ–ö–ò –ù–ê –ü–û–õ–ù–û–†–ê–ó–ú–ï–†–ù–´–ï –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø
        gallery_links = soup.select('a.gallery__link[href]')
        for link in gallery_links:
            href = link.get('href', '')
            if any(ext in href.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                img_url = urljoin(page_url, href)
                if img_url not in image_urls:
                    image_urls.append(img_url)

        # 4. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô –ü–û–ò–°–ö - –ª—é–±—ã–µ img –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
        content_imgs = soup.select('div.content img, div.harmful-detail img, img')
        for img in content_imgs:
            if img.get('src'):
                img_url = urljoin(page_url, img['src'])
                if img_url not in image_urls and any(
                        ext in img_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                    image_urls.append(img_url)

        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(image_urls)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        return image_urls

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
        return []


def parse_content_from_html(soup):
    """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –†–ï–ê–õ–¨–ù–û–ô —Å—Ç—Ä—É–∫—Ç—É—Ä—ã HTML"""
    try:
        # –ù–ê–ó–í–ê–ù–ò–ï –∏–∑ H1
        title_elem = soup.find('h1')
        title = title_elem.get_text(strip=True) if title_elem else ""

        # –ù–ê–£–ß–ù–û–ï –ù–ê–ó–í–ê–ù–ò–ï
        scientific_name = ""
        detail_text = soup.find('div', class_='harmful-detail__text')
        if detail_text:
            italic_text = detail_text.find('i')
            if italic_text:
                scientific_name = italic_text.get_text(strip=True)

        # –í–ï–°–¨ –¢–ï–ö–°–¢–û–í–´–ô –ö–û–ù–¢–ï–ù–¢
        all_content = ""
        if detail_text:
            paragraphs = detail_text.find_all(['p', 'div', 'h3'])
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 10:
                    all_content += text + "\n\n"

        if not all_content:
            content_area = soup.find('div', class_='content__inner') or soup.find('main')
            if content_area:
                all_content = content_area.get_text(separator='\n\n', strip=True)

        # –†–ê–ó–ë–ò–í–ê–ï–ú –ö–û–ù–¢–ï–ù–¢ –ù–ê –°–ï–ö–¶–ò–ò
        content_lines = all_content.split('\n\n')

        description_ru = ""
        symptoms_ru = ""
        development_conditions_ru = ""
        control_measures_ru = ""

        current_section = "description"

        for line in content_lines:
            line = line.strip()
            if len(line) < 20:
                continue

            line_lower = line.lower()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–∫—Ü–∏—é –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
            if any(keyword in line_lower for keyword in ['—Å–∏–º–ø—Ç–æ–º—ã –±–æ–ª–µ–∑–Ω–∏', '—Å–∏–º–ø—Ç–æ–º—ã', '–ø—Ä–∏–∑–Ω–∞–∫–∏ –±–æ–ª–µ–∑–Ω–∏']):
                current_section = "symptoms"
                continue
            elif any(keyword in line_lower for keyword in ['—Ñ–∞–∫—Ç–æ—Ä—ã', '—É—Å–ª–æ–≤–∏—è —Ä–∞–∑–≤–∏—Ç–∏—è', '—Ä–∞–∑–≤–∏—Ç–∏–µ –±–æ–ª–µ–∑–Ω–∏']):
                current_section = "development"
                continue
            elif any(keyword in line_lower for keyword in ['–º–µ—Ä—ã –∑–∞—â–∏—Ç—ã', '–º–µ—Ä—ã –±–æ—Ä—å–±—ã', '–∑–∞—â–∏—Ç–∞']):
                current_section = "control"
                continue

            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π —Å–µ–∫—Ü–∏–∏
            if current_section == "description":
                description_ru += line + " "
            elif current_section == "symptoms":
                symptoms_ru += line + " "
            elif current_section == "development":
                development_conditions_ru += line + " "
            elif current_section == "control":
                control_measures_ru += line + " "

        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç—å, –∫–ª–∞–¥–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –≤ –æ–ø–∏—Å–∞–Ω–∏–µ
        if not any([symptoms_ru, development_conditions_ru, control_measures_ru]):
            description_ru = all_content[:1500] if all_content else ""

        return {
            'title': title,
            'scientific_name': scientific_name,
            'description_ru': description_ru.strip(),
            'symptoms_ru': symptoms_ru.strip(),
            'development_conditions_ru': development_conditions_ru.strip(),
            'control_measures_ru': control_measures_ru.strip()
        }

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
        return None


def get_subcategory_links(main_category_url):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–∫—É–ª—å—Ç—É—Ä—ã) –∏–ª–∏ –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–æ—Ä–Ω—è–∫–∏"""
    driver = get_webdriver()

    try:
        logger.info(f"üîó –ò—â–µ–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤: {main_category_url}")
        driver.get(main_category_url)

        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        debug_file = f"debug_{main_category_url.split('/')[-2]}_{uuid.uuid4().hex[:8]}.html"
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(driver.page_source)
        logger.info(f"üîç HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {debug_file}")

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        subcategories = []

        # –î–õ–Ø –°–û–†–ù–Ø–ö–û–í - –∏—â–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if 'sornyaki' in main_category_url:
            logger.info("üå± –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ—Ä–Ω—è–∫–∏ - –∏—â–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏")

            # –ò–∑ HTML –≤–∏–¥–Ω–æ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–ª–∞—Å—Å .agro-item —Å —Å—Å—ã–ª–∫–æ–π .title
            agro_items = soup.select('div.agro-item')
            for item in agro_items:
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–∞
                link = item.find('a', class_='title') or item.find('a')
                if link and link.get('href'):
                    href = link.get('href', '')
                    text = link.get_text(strip=True)

                    # –§–∏–ª—å—Ç—Ä –¥–ª—è —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–æ—Ä–Ω—è–∫–∏
                    if (href.startswith('/harmful/sornyaki/') and
                            not href.endswith('/') and  # –ù–ï –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                            len(text) > 3 and
                            not any(exclude in text.lower() for exclude in
                                    ['–≥–ª–∞–≤–Ω–∞—è', '–Ω–∞–∑–∞–¥', '–º–µ–Ω—é', '–ø–æ–∏—Å–∫', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '–∫–∞—Ç–∞–ª–æ–≥'])):
                        detail_url = urljoin(BASE_URL, href)
                        subcategories.append({
                            'name': text,
                            'url': detail_url
                        })
                        logger.info(f"üå± –ù–∞–π–¥–µ–Ω —Å–æ—Ä–Ω—è–∫: {text}")

        # –î–õ–Ø –ë–û–õ–ï–ó–ù–ï–ô –ò –í–†–ï–î–ò–¢–ï–õ–ï–ô - –∏—â–µ–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–∫—É–ª—å—Ç—É—Ä—ã)
        else:
            logger.info("ü¶†üêõ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–æ–ª–µ–∑–Ω–∏/–≤—Ä–µ–¥–∏—Ç–µ–ª–∏ - –∏—â–µ–º –∫—É–ª—å—Ç—É—Ä—ã")

            # –ò–∑ HTML –≤–∏–¥–Ω–æ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–ª–∞—Å—Å .agro-item —Å —Å—Å—ã–ª–∫–æ–π .title
            agro_items = soup.select('div.agro-item')
            for item in agro_items:
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–∞
                link = item.find('a', class_='title') or item.find('a')
                if link and link.get('href'):
                    href = link.get('href', '')
                    text = link.get_text(strip=True)

                    if (href.startswith('/harmful/') and
                            href.endswith('/') and
                            len(text) > 5 and
                            not any(exclude in text.lower() for exclude in
                                    ['–≥–ª–∞–≤–Ω–∞—è', '–Ω–∞–∑–∞–¥', '–º–µ–Ω—é', '–ø–æ–∏—Å–∫', '–∫–æ–Ω—Ç–∞–∫—Ç—ã'])):

                        subcategory_url = urljoin(BASE_URL, href)
                        if not any(sub['url'] == subcategory_url for sub in subcategories):
                            subcategories.append({
                                'name': text,
                                'url': subcategory_url
                            })
                            logger.info(f"üìÇ –ù–∞–π–¥–µ–Ω–∞ –∫—É–ª—å—Ç—É—Ä–∞: {text}")

        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(subcategories)} –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π/—ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        return subcategories

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
        return []
    finally:
        driver.quit()


def get_detail_links_from_culture(culture_url):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –∏–∑ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫—É–ª—å—Ç—É—Ä—ã"""
    driver = get_webdriver()

    try:
        logger.info(f"üîç –ò—â–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤: {culture_url}")
        driver.get(culture_url)

        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        debug_file = f"debug_culture_{uuid.uuid4().hex[:8]}.html"
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(driver.page_source)
        logger.info(f"üîç HTML –∫—É–ª—å—Ç—É—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {debug_file}")

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        detail_links = []

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø (–±–æ–ª–µ–∑–Ω–∏ –∏–ª–∏ –≤—Ä–µ–¥–∏—Ç–µ–ª–∏) –∏–∑ URL
        is_diseases = 'bolezni' in culture_url
        is_pests = 'vrediteli' in culture_url

        # –ò—â–µ–º –≤—Å–µ agro-item —ç–ª–µ–º–µ–Ω—Ç—ã
        agro_items = soup.select('div.agro-item')

        for item in agro_items:
            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏ —ç–ª–µ–º–µ–Ω—Ç–∞
            link = item.find('a', class_='title') or item.find('a')
            if link and link.get('href'):
                href = link.get('href', '')
                text = link.get_text(strip=True)

                # –§–∏–ª—å—Ç—Ä –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                if (href.startswith('/harmful/') and
                        not href.endswith('/') and  # –ù–ï –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                        len(text) > 5 and
                        href != culture_url.replace(BASE_URL, '') and
                        not any(exclude in text.lower() for exclude in
                                ['–≥–ª–∞–≤–Ω–∞—è', '–Ω–∞–∑–∞–¥', '–º–µ–Ω—é', '–ø–æ–∏—Å–∫', '–∑–∞–∫–∞–∑–∞—Ç—å', '–ø–æ–¥—Ä–æ–±–Ω–µ–µ', '–∫–æ–Ω—Ç–∞–∫—Ç—ã'])):

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–∏–ø—É
                    if ((is_diseases and 'bolezni' in href) or
                            (is_pests and 'vrediteli' in href)):
                        detail_url = urljoin(BASE_URL, href)
                        detail_links.append({
                            'name': text,
                            'url': detail_url
                        })
                        logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–∞ –¥–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {text}")

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        seen_urls = set()
        unique_links = []
        for link in detail_links:
            if link['url'] not in seen_urls:
                seen_urls.add(link['url'])
                unique_links.append(link)

        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(unique_links)} –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")
        return unique_links

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {e}")
        return []
    finally:
        driver.quit()


def scrape_detail_page(page_url, category_type):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    driver = get_webdriver()

    try:
        logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {page_url}")
        driver.get(page_url)

        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # –ü–∞—Ä—Å–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç
        content = parse_content_from_html(soup)
        if not content or not content['title']:
            logger.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å {page_url}")
            return None

        logger.info(f"üìù –ù–∞–π–¥–µ–Ω: {content['title']}")

        # –ü–µ—Ä–µ–≤–æ–¥—ã (–µ—Å–ª–∏ –µ—Å—Ç—å API –∫–ª—é—á)
        translations = {}
        if OPENAI_API_KEY:
            logger.info("üåç –í—ã–ø–æ–ª–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã...")
            for field in ['description_ru', 'symptoms_ru', 'development_conditions_ru', 'control_measures_ru']:
                if content[field]:
                    translations[field.replace('_ru', '_ua')] = translate_text_gpt(content[field], "ukrainian")
                    translations[field.replace('_ru', '_en')] = translate_text_gpt(content[field], "english")
                    time.sleep(0.5)
                else:
                    translations[field.replace('_ru', '_ua')] = ""
                    translations[field.replace('_ru', '_en')] = ""

            name_en = translate_text_gpt(content['title'], "english")
        else:
            logger.warning("‚ö†Ô∏è API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã")
            for field in ['description_ua', 'description_en', 'symptoms_ua', 'symptoms_en',
                          'development_conditions_ua', 'development_conditions_en',
                          'control_measures_ua', 'control_measures_en']:
                translations[field] = ""
            name_en = ""

        # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        logger.info("üì∏ –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
        image_urls = extract_images_from_page(soup, page_url)

        item_id = str(uuid.uuid4())
        downloaded_images = []

        if image_urls:
            for i, image_url in enumerate(image_urls):
                file_extension = '.jpg'
                try:
                    parsed_url = urlparse(image_url)
                    file_extension = os.path.splitext(parsed_url.path)[1] or '.jpg'
                except:
                    pass

                safe_title = "".join(c for c in content['title'] if c.isalnum() or c in (' ', '-', '_')).strip()
                safe_title = safe_title.replace(' ', '_')[:30]

                if len(image_urls) == 1:
                    filename = f"{category_type}_{safe_title}_{item_id[:8]}{file_extension}"
                else:
                    filename = f"{category_type}_{safe_title}_{item_id[:8]}_{i + 1:02d}{file_extension}"

                filepath = os.path.join(IMAGES_DIR, category_type, filename)

                if download_image(image_url, filepath, page_url):
                    downloaded_images.append({
                        'image_url': image_url,
                        'image_path': filepath,
                        'filename': filename
                    })

                time.sleep(random.uniform(0.5, 1.5))

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫—É–ª—å—Ç—É—Ä—ã
        text_for_crops = (content['description_ru'] + ' ' + content['symptoms_ru']).lower()
        crops = []
        crop_keywords = ['–ø—à–µ–Ω–∏—Ü–∞', '—Ä–æ–∂—å', '—è—á–º–µ–Ω—å', '–æ–≤–µ—Å', '–∫—É–∫—É—Ä—É–∑–∞', '–ø–æ–¥—Å–æ–ª–Ω–µ—á–Ω–∏–∫', '—Å–æ—è', '—Ä–∞–ø—Å', '—Å–≤–µ–∫–ª–∞',
                         '–∫–∞—Ä—Ç–æ—Ñ–µ–ª—å']

        for crop in crop_keywords:
            if crop in text_for_crops:
                crops.append(crop)

        if not crops:
            crops = ['–ø—à–µ–Ω–∏—Ü–∞']

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            'id': item_id,
            'name': content['title'],
            'name_en': name_en,
            'scientific_name': content['scientific_name'],
            'description_ru': content['description_ru'],
            'description_ua': translations.get('description_ua', ''),
            'description_en': translations.get('description_en', ''),
            'symptoms_ru': content['symptoms_ru'],
            'symptoms_ua': translations.get('symptoms_ua', ''),
            'symptoms_en': translations.get('symptoms_en', ''),
            'development_conditions_ru': content['development_conditions_ru'],
            'development_conditions_ua': translations.get('development_conditions_ua', ''),
            'development_conditions_en': translations.get('development_conditions_en', ''),
            'control_measures_ru': content['control_measures_ru'],
            'control_measures_ua': translations.get('control_measures_ua', ''),
            'control_measures_en': translations.get('control_measures_en', ''),
            'source_urls': page_url,
            'crops': crops,
            'images': downloaded_images,
            'is_active': True,
            'version': 1
        }

        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {content['title']} ({len(downloaded_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)")
        return result

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {page_url}: {e}")
        return None
    finally:
        driver.quit()


def save_data_to_csv(data, category_type):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ CSV —Ñ–∞–π–ª—ã"""
    if not data:
        logger.warning(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ {category_type}")
        return

    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º {len(data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è {category_type}")

    table_mapping = {
        'diseases': 'disease',
        'pests': 'vermin',
        'weeds': 'weed'
    }

    table_name = table_mapping.get(category_type, category_type[:-1])

    # –§–∞–π–ª—ã CSV
    main_csv = os.path.join(OUTPUT_DIR, f'{category_type}.csv')
    descriptions_csv = os.path.join(OUTPUT_DIR, f'{table_name}_descriptions.csv')
    images_csv = os.path.join(OUTPUT_DIR, f'{table_name}_images.csv')
    crops_csv = os.path.join(OUTPUT_DIR, f'{table_name}_crops.csv')

    # 1. –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    with open(main_csv, 'w', encoding='utf-8', newline='') as f:
        fieldnames = ['id', 'name', 'name_en', 'scientific_name', 'is_active']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for item in data:
            writer.writerow({
                'id': item['id'],
                'name': item['name'],
                'name_en': item['name_en'],
                'scientific_name': item['scientific_name'],
                'is_active': item['is_active']
            })

    # 2. –û–ø–∏—Å–∞–Ω–∏—è
    with open(descriptions_csv, 'w', encoding='utf-8', newline='') as f:
        if category_type == 'diseases':
            fieldnames = [
                'id', 'disease_id', 'description_ru', 'description_ua', 'description_en',
                'symptoms_ru', 'symptoms_ua', 'symptoms_en',
                'development_conditions_ru', 'development_conditions_ua', 'development_conditions_en',
                'control_measures_ru', 'control_measures_ua', 'control_measures_en',
                'photo_path', 'source_urls', 'version'
            ]
        elif category_type == 'pests':
            fieldnames = [
                'id', 'vermin_id', 'description_ru', 'description_ua', 'description_en',
                'damage_symptoms_ru', 'damage_symptoms_ua', 'damage_symptoms_en',
                'biology_ru', 'biology_ua', 'biology_en',
                'control_measures_ru', 'control_measures_ua', 'control_measures_en',
                'photo_path', 'source_urls', 'version'
            ]
        else:  # weeds
            fieldnames = [
                'id', 'weed_id', 'description_ru', 'description_ua', 'description_en',
                'biological_features_ru', 'biological_features_ua', 'biological_features_en',
                'harmfulness_ru', 'harmfulness_ua', 'harmfulness_en',
                'control_measures_ru', 'control_measures_ua', 'control_measures_en',
                'photo_path', 'source_urls', 'version'
            ]

        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for item in data:
            main_photo_path = item['images'][0]['image_path'] if item['images'] else ""

            base_row = {
                'id': str(uuid.uuid4()),
                f'{table_name}_id': item['id'],
                'description_ru': item['description_ru'],
                'description_ua': item['description_ua'],
                'description_en': item['description_en'],
                'control_measures_ru': item['control_measures_ru'],
                'control_measures_ua': item['control_measures_ua'],
                'control_measures_en': item['control_measures_en'],
                'photo_path': main_photo_path,
                'source_urls': item['source_urls'],
                'version': item['version']
            }

            if category_type == 'diseases':
                base_row.update({
                    'symptoms_ru': item['symptoms_ru'],
                    'symptoms_ua': item['symptoms_ua'],
                    'symptoms_en': item['symptoms_en'],
                    'development_conditions_ru': item['development_conditions_ru'],
                    'development_conditions_ua': item['development_conditions_ua'],
                    'development_conditions_en': item['development_conditions_en']
                })
            elif category_type == 'pests':
                base_row.update({
                    'damage_symptoms_ru': item['symptoms_ru'],
                    'damage_symptoms_ua': item['symptoms_ua'],
                    'damage_symptoms_en': item['symptoms_en'],
                    'biology_ru': item['development_conditions_ru'],
                    'biology_ua': item['development_conditions_ua'],
                    'biology_en': item['development_conditions_en']
                })
            else:  # weeds
                base_row.update({
                    'biological_features_ru': item['symptoms_ru'],
                    'biological_features_ua': item['symptoms_ua'],
                    'biological_features_en': item['symptoms_en'],
                    'harmfulness_ru': item['development_conditions_ru'],
                    'harmfulness_ua': item['development_conditions_ua'],
                    'harmfulness_en': item['development_conditions_en']
                })

            writer.writerow(base_row)

    # 3. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    with open(images_csv, 'w', encoding='utf-8', newline='') as f:
        fieldnames = ['id', f'{table_name}_id', 'image_path', 'image_url', 'version']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for item in data:
            for image in item['images']:
                writer.writerow({
                    'id': str(uuid.uuid4()),
                    f'{table_name}_id': item['id'],
                    'image_path': image['image_path'],
                    'image_url': image['image_url'],
                    'version': 1
                })

    # 4. –ö—É–ª—å—Ç—É—Ä—ã
    with open(crops_csv, 'w', encoding='utf-8', newline='') as f:
        fieldnames = [f'{table_name}_id', 'crops']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for item in data:
            for crop in item['crops']:
                writer.writerow({
                    f'{table_name}_id': item['id'],
                    'crops': crop
                })

    logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 4 CSV —Ñ–∞–π–ª–∞ –¥–ª—è {category_type}")


def process_category_universal(category_type, max_items=None):
    """–£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    logger.info(f"üöÄ –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞: {category_type}")

    # URL –≥–ª–∞–≤–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    main_urls = {
        'diseases': 'https://betaren.ru/harmful/bolezni/',
        'pests': 'https://betaren.ru/harmful/vrediteli/',
        'weeds': 'https://betaren.ru/harmful/sornyaki/'
    }

    main_url = main_urls.get(category_type)
    if not main_url:
        logger.error(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {category_type}")
        return []

    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–ª–∏ –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏
    subcategories = get_subcategory_links(main_url)

    if not subcategories:
        logger.error(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è {category_type}")
        return []

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
    all_detail_links = []

    for subcat in subcategories:
        if category_type == 'weeds':
            # –î–ª—è —Å–æ—Ä–Ω—è–∫–æ–≤ - —ç—Ç–æ —É–∂–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            all_detail_links.append(subcat)
        else:
            # –î–ª—è –±–æ–ª–µ–∑–Ω–µ–π –∏ –≤—Ä–µ–¥–∏—Ç–µ–ª–µ–π - –ø–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ –∫—É–ª—å—Ç—É—Ä
            detail_links = get_detail_links_from_culture(subcat['url'])
            all_detail_links.extend(detail_links)
            time.sleep(1)

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if max_items and len(all_detail_links) > max_items:
        all_detail_links = all_detail_links[:max_items]
        logger.info(f"‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ {max_items} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")

    logger.info(f"üìä –ë—É–¥–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å {len(all_detail_links)} —Å—Å—ã–ª–æ–∫ –¥–ª—è {category_type}")

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É
    processed_data = []

    for i, link in enumerate(all_detail_links, 1):
        logger.info(f"üìÑ [{i}/{len(all_detail_links)}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {link['name']}")

        try:
            result = scrape_detail_page(link['url'], category_type)

            if result:
                processed_data.append(result)
                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {result['name']}")

                # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                if len(processed_data) % 5 == 0:
                    save_data_to_csv(processed_data, category_type)
            else:
                logger.warning(f"‚ùå –ù–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {link['name']}")

            time.sleep(SLEEP_BETWEEN_REQUESTS)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            continue

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    if processed_data:
        save_data_to_csv(processed_data, category_type)
        logger.info(f"üéâ {category_type} –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed_data)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    else:
        logger.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–ª—è {category_type}")

    return processed_data


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üîß –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô —Å–∫—Ä–∞–ø–µ—Ä Betaren.ru")
    print("   ‚úÖ –û–±—Ö–æ–¥–∏—Ç –í–°–ï –∫—É–ª—å—Ç—É—Ä—ã –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
    print("   ‚úÖ –°–æ–±–∏—Ä–∞–µ—Ç –í–°–ï –±–æ–ª–µ–∑–Ω–∏, –≤—Ä–µ–¥–∏—Ç–µ–ª–∏, —Å–æ—Ä–Ω—è–∫–∏")
    print("   ‚úÖ –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    print("   ‚úÖ –°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω—ã–µ CSV —Ñ–∞–π–ª—ã")
    print()
    print("1. –ë–û–õ–ï–ó–ù–ò - –ø–æ–ª–Ω—ã–π —Å–±–æ—Ä –ø–æ –≤—Å–µ–º –∫—É–ª—å—Ç—É—Ä–∞–º")
    print("2. –í–†–ï–î–ò–¢–ï–õ–ò - –ø–æ–ª–Ω—ã–π —Å–±–æ—Ä –ø–æ –≤—Å–µ–º –∫—É–ª—å—Ç—É—Ä–∞–º")
    print("3. –°–û–†–ù–Ø–ö–ò - –ø–æ–ª–Ω—ã–π —Å–±–æ—Ä")
    print("4. –í–°–ï –ö–ê–¢–ï–ì–û–†–ò–ò - –ø–æ–ª–Ω—ã–π —Å–±–æ—Ä")
    print("5. –¢–ï–°–¢: –ø–æ 3 —ç–ª–µ–º–µ–Ω—Ç–∞ –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")

    choice = input("\n–í—ã–±–µ—Ä–∏—Ç–µ –æ–ø—Ü–∏—é (1-5): ").strip()

    if not OPENAI_API_KEY:
        logger.warning("‚ö†Ô∏è OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω - –ø–µ—Ä–µ–≤–æ–¥—ã –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã")
        proceed = input("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –±–µ–∑ –ø–µ—Ä–µ–≤–æ–¥–æ–≤? (y/n): ").strip().lower()
        if proceed != 'y':
            return

    create_directories()

    if choice == "1":
        logger.info("ü¶† –ü–û–õ–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ë–û–õ–ï–ó–ù–ï–ô")
        process_category_universal('diseases')

    elif choice == "2":
        logger.info("üêõ –ü–û–õ–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ –í–†–ï–î–ò–¢–ï–õ–ï–ô")
        process_category_universal('pests')

    elif choice == "3":
        logger.info("üå± –ü–û–õ–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ –°–û–†–ù–Ø–ö–û–í")
        process_category_universal('weeds')

    elif choice == "4":
        logger.info("üöÄ –ü–û–õ–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ –í–°–ï–• –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
        process_category_universal('diseases')
        process_category_universal('pests')
        process_category_universal('weeds')

    elif choice == "5":
        logger.info("üß™ –¢–ï–°–¢–û–í–´–ô —Ä–µ–∂–∏–º")
        process_category_universal('diseases', max_items=3)
        process_category_universal('pests', max_items=3)
        process_category_universal('weeds', max_items=3)

    else:
        print("‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≤—ã–±–æ—Ä")
        return

    print(f"\nüéâ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫–µ: {OUTPUT_DIR}")
    print(f"üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–∞–ø–∫–µ: {IMAGES_DIR}")


if __name__ == "__main__":
    main()