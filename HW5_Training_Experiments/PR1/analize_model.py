#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image
import pandas as pd
import numpy as np
from collections import Counter


def analyze_trained_model():
    """–ê–Ω–∞–ª—ñ–∑ –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ"""

    print("üîç –ê–ù–ê–õ–Ü–ó –ù–ê–í–ß–ï–ù–û–á –ú–û–î–ï–õ–Ü")
    print("=" * 50)

    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ
    metadata_path = "models/simple_metadata.json"
    if os.path.exists(metadata_path):
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)

        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ú–û–î–ï–õ–Ü:")
        print(f"  –ù–∞–∑–≤–∞ –º–æ–¥–µ–ª—ñ: {metadata['model_name']}")
        print(f"  –ù–∞–π–∫—Ä–∞—â–∏–π F1-—Å–∫–æ—Ä: {metadata['best_f1']:.3f}")
        print(f"  –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤: {metadata['num_classes']}")
        print(f"  –†–æ–∑–º—ñ—Ä –¥–∞—Ç–∞—Å–µ—Ç—É: {metadata['dataset_size']} –∑–æ–±—Ä–∞–∂–µ–Ω—å")

        print(f"\nüè∑Ô∏è –ö–õ–ê–°–ò, –Ø–ö–Ü –†–û–ó–ü–Ü–ó–ù–ê–Ñ –ú–û–î–ï–õ–¨:")
        class_names = metadata['class_names']

        for i, class_name in enumerate(class_names):
            print(f"  {i:2d}. {class_name}")

        print(f"\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–ò:")
        print(f"  F1-—Å–∫–æ—Ä 82.7% –æ–∑–Ω–∞—á–∞—î:")
        print(f"  ‚úÖ –ú–æ–¥–µ–ª—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–æ–∑–ø—ñ–∑–Ω–∞—î ~83% —Ö–≤–æ—Ä–æ–±")
        print(f"  ‚úÖ –¶–µ –¥—É–∂–µ —Ö–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó!")
        print(f"  ‚úÖ –ü—Ä–∏–¥–∞—Ç–Ω–∞ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–Ω–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è")

    else:
        print("‚ùå –ú–µ—Ç–∞–¥–∞–Ω—ñ –º–æ–¥–µ–ª—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")

    # –ê–Ω–∞–ª—ñ–∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
    print(f"\nüå± –ê–ù–ê–õ–Ü–ó –û–†–ò–ì–Ü–ù–ê–õ–¨–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–£:")
    analyze_original_dataset()


def analyze_original_dataset():
    """–ê–Ω–∞–ª—ñ–∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É"""

    try:
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ
        diseases_path = "../crawler/downloads/diseases.csv"
        images_path = "../crawler/downloads/disease_images.csv"

        if os.path.exists(diseases_path) and os.path.exists(images_path):
            diseases_df = pd.read_csv(diseases_path)
            images_df = pd.read_csv(images_path)

            print(f"  üìÅ –ó–∞–≥–∞–ª–æ–º —Ö–≤–æ—Ä–æ–± —É –±–∞–∑—ñ: {len(diseases_df)}")
            print(f"  üñºÔ∏è –ó–∞–≥–∞–ª–æ–º –∑–æ–±—Ä–∞–∂–µ–Ω—å: {len(images_df)}")

            # –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –ø–æ —Ö–≤–æ—Ä–æ–±–∞—Ö
            disease_counts = {}

            for _, row in images_df.iterrows():
                disease_id = row["disease_id"]
                disease_info = diseases_df[diseases_df["id"] == disease_id]

                if not disease_info.empty:
                    disease_name = disease_info.iloc[0]["name"]
                    disease_counts[disease_name] = disease_counts.get(disease_name, 0) + 1

            print(f"\n  üìä –¢–û–ü-10 –•–í–û–†–û–ë –ó–ê –ö–Ü–õ–¨–ö–Ü–°–¢–Æ –ó–û–ë–†–ê–ñ–ï–ù–¨:")
            sorted_diseases = sorted(disease_counts.items(), key=lambda x: x[1], reverse=True)

            for i, (disease, count) in enumerate(sorted_diseases[:10]):
                print(f"    {i + 1:2d}. {disease}: {count} –∑–æ–±—Ä–∞–∂–µ–Ω—å")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ
            print(f"\n  üéØ –ù–ê–í–ß–ï–ù–ê –ú–û–î–ï–õ–¨ –í–ò–ö–û–†–ò–°–¢–û–í–£–Ñ:")
            print(f"    ‚Ä¢ –¢—ñ–ª—å–∫–∏ —Ö–≤–æ—Ä–æ–±–∏ –∑ ‚â•2 –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è–º–∏")
            print(f"    ‚Ä¢ –§—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω—ñ —Ç–∞ –æ—á–∏—â–µ–Ω—ñ –¥–∞–Ω—ñ")
            print(f"    ‚Ä¢ –ü–µ—Ä–µ–º–∞–ø–ª–µ–Ω–Ω—ñ –∫–ª–∞—Å–∏ (0, 1, 2, ...)")

        else:
            print("  ‚ùå –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ CSV —Ñ–∞–π–ª–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")

    except Exception as e:
        print(f"  ‚ùå –ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É: {e}")


def show_class_distribution():
    """–ü–æ–∫–∞–∑—É—î —Ä–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤ —É –Ω–∞–≤—á–µ–Ω—ñ–π –º–æ–¥–µ–ª—ñ"""

    print(f"\nüìà –†–û–ó–ü–û–î–Ü–õ –ö–õ–ê–°–Ü–í –£ –ù–ê–í–ß–ï–ù–Ü–ô –ú–û–î–ï–õ–Ü:")

    # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ —è–∫—ñ –∫–ª–∞—Å–∏ –ø–æ—Ç—Ä–∞–ø–∏–ª–∏ –¥–æ –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ
    try:
        diseases_df = pd.read_csv("../crawler/downloads/diseases.csv")
        images_df = pd.read_csv("../crawler/downloads/disease_images.csv")

        # –†–∞—Ö—É—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –ø–æ —Ö–≤–æ—Ä–æ–±–∞—Ö
        disease_counts = {}
        for _, row in images_df.iterrows():
            disease_id = row["disease_id"]
            disease_info = diseases_df[diseases_df["id"] == disease_id]

            if not disease_info.empty:
                disease_name = disease_info.iloc[0]["name"]
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —ñ—Å–Ω—É—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
                image_path = row["image_path"]
                clean_path = image_path.replace("downloads\\images/", "").replace("downloads/images/", "")
                full_path = os.path.join("../crawler/downloads/images/", clean_path)

                if os.path.exists(full_path):
                    disease_counts[disease_name] = disease_counts.get(disease_name, 0) + 1

        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —è–∫ —É –Ω–∞–≤—á–µ–Ω—ñ–π –º–æ–¥–µ–ª—ñ (‚â•2 –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è)
        valid_diseases = {name: count for name, count in disease_counts.items() if count >= 2}

        print(f"  –í–∞–ª—ñ–¥–Ω–∏—Ö —Ö–≤–æ—Ä–æ–±: {len(valid_diseases)}")
        print(f"  –ó–∞–≥–∞–ª–æ–º –∑–æ–±—Ä–∞–∂–µ–Ω—å: {sum(valid_diseases.values())}")

        # –ü–æ–∫–∞–∑—É—î–º–æ —Ä–æ–∑–ø–æ–¥—ñ–ª
        sorted_valid = sorted(valid_diseases.items(), key=lambda x: x[1], reverse=True)

        print(f"\n  –†–û–ó–ü–û–î–Ü–õ –ü–û –ö–Ü–õ–¨–ö–û–°–¢–Ü –ó–û–ë–†–ê–ñ–ï–ù–¨:")
        for disease, count in sorted_valid:
            bar = "‚ñà" * min(count, 20)  # –í—ñ–∑—É–∞–ª—å–Ω–∞ —à–∫–∞–ª–∞
            print(f"    {disease[:30]:30s} | {count:2d} | {bar}")

    except Exception as e:
        print(f"  ‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")


def predict_example():
    """–ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""

    print(f"\nüîÆ –ü–†–ò–ö–õ–ê–î –í–ò–ö–û–†–ò–°–¢–ê–ù–ù–Ø –ú–û–î–ï–õ–Ü:")
    print("```python")
    print("import torch")
    print("from torchvision import models, transforms")
    print("from PIL import Image")
    print("")
    print("# –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å")
    print("model = models.mobilenet_v2()")
    print("model.classifier[1] = torch.nn.Linear(1280, num_classes)")
    print("model.load_state_dict(torch.load('models/simple_best_model.pt'))")
    print("model.eval()")
    print("")
    print("# –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è")
    print("transform = transforms.Compose([")
    print("    transforms.Resize((224, 224)),")
    print("    transforms.ToTensor(),")
    print("    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])")
    print("])")
    print("")
    print("# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è")
    print("image = Image.open('path_to_image.jpg')")
    print("input_tensor = transform(image).unsqueeze(0)")
    print("with torch.no_grad():")
    print("    output = model(input_tensor)")
    print("    predicted_class = torch.argmax(output, dim=1)")
    print("    disease_name = class_names[predicted_class]")
    print("```")


def main():
    analyze_trained_model()
    show_class_distribution()
    predict_example()

    print(f"\nüéØ –í–ò–°–ù–û–í–û–ö:")
    print(f"–í–∞—à–∞ –º–æ–¥–µ–ª—å —É—Å–ø—ñ—à–Ω–æ –Ω–∞–≤—á–∏–ª–∞—Å—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞—Ç–∏ —Ö–≤–æ—Ä–æ–±–∏ —Ä–æ—Å–ª–∏–Ω")
    print(f"–∑ —Ç–æ—á–Ω—ñ—Å—Ç—é 82.7% - —Ü–µ –≤—ñ–¥–º—ñ–Ω–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!")
    print(f"–ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–Ω–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è! üöÄ")


if __name__ == "__main__":
    main()