#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
import torch.nn.functional as F
import pandas as pd
import numpy as np
import wandb
import json
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
from tqdm import tqdm
import random

# –®–í–ò–î–ö–ê –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è –º–∞–ª–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤
CONFIG = {
    "data_path": "../crawler/downloads/",
    "image_dir": "../crawler/downloads/images/",
    "model_name": "mobilenet_v2",  # –®–≤–∏–¥–∫–∞ –º–æ–¥–µ–ª—å
    "num_epochs": 30,  # –ú–µ–Ω—à–µ –µ–ø–æ—Ö
    "batch_size": 8,  # –ú–µ–Ω—à–∏–π batch –¥–ª—è –º–∞–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
    "learning_rate": 0.001,  # –í–∏—â–∏–π LR –¥–ª—è —à–≤–∏–¥—à–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
    "weight_decay": 1e-4,  # –ú–µ–Ω—à–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
    "dropout": 0.3,  # –ú–µ–Ω—à–∏–π dropout
    "seed": 42,
    "validation_split": 0.3,  # –ë—ñ–ª—å—à–µ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö
    "risk_type": "diseases",
    "patience": 8,  # –ú–µ–Ω—à–∞ patience
    "label_smoothing": 0.05,  # –ú–µ–Ω—à–µ smoothing

    # –í–ò–ú–ö–ù–£–¢–û –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
    "progressive_resizing": False,  # –í–∏–º–∫–Ω—É—Ç–æ
    "use_mixup": False,  # –í–∏–º–∫–Ω—É—Ç–æ
    "use_cutmix": False,  # –í–∏–º–∫–Ω—É—Ç–æ
    "use_focal_loss": False,  # –í–∏–º–∫–Ω—É—Ç–æ
    "use_cosine_annealing": False,  # –í–∏–º–∫–Ω—É—Ç–æ
    "gradual_unfreezing": False,  # –í–∏–º–∫–Ω—É—Ç–æ
    "use_tta": False,  # –í–ò–ú–ö–ù–£–¢–û TTA!

    # –ü—Ä–æ—Å—Ç—ñ—à—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    "image_size": 224,
    "freeze_epochs": 3,  # –ó–∞–º–æ—Ä–æ–∑–∏—Ç–∏ backbone –Ω–∞ 3 –µ–ø–æ—Ö–∏
}

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è seed
torch.manual_seed(CONFIG["seed"])
np.random.seed(CONFIG["seed"])
random.seed(CONFIG["seed"])


class FastAgriculturalRiskDataset(Dataset):
    def __init__(self, csv_file, image_dir, transform=None, risk_type="diseases"):
        """–®–≤–∏–¥–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –±–µ–∑ –∑–∞–π–≤–∏—Ö –ø–µ—Ä–µ–≤—ñ—Ä–æ–∫"""
        self.image_dir = image_dir
        self.transform = transform
        self.risk_type = risk_type

        print(f"–®–≤–∏–¥–∫–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É: {risk_type}")

        try:
            if risk_type == "diseases":
                self.risks_df = pd.read_csv(os.path.join(csv_file, "diseases.csv"))
                self.images_df = pd.read_csv(os.path.join(csv_file, "disease_images.csv"))

            print(f"–†–∏–∑–∏–∫—ñ–≤: {len(self.risks_df)}, –ó–æ–±—Ä–∞–∂–µ–Ω—å: {len(self.images_df)}")

        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}")
            self.data = []
            self.classes = []
            self.class_to_idx = {}
            return

        self.classes = self.risks_df["name"].unique()
        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}
        print(f"–ö–ª–∞—Å—ñ–≤: {len(self.classes)}")

        # –®–≤–∏–¥–∫–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
        self.data = []
        valid_images = 0

        for _, row in self.images_df.iterrows():
            try:
                risk_id = row["disease_id"]
                risk_matches = self.risks_df[self.risks_df["id"] == risk_id]

                if risk_matches.empty:
                    continue

                risk_row = risk_matches.iloc[0]
                class_name = risk_row["name"]
                image_path = row["image_path"]

                # –ü—Ä–æ—Å—Ç—ñ—à–∞ –æ–±—Ä–æ–±–∫–∞ —à–ª—è—Ö—ñ–≤
                clean_path = image_path.replace("downloads\\images/", "").replace("downloads/images/", "")
                full_path = os.path.join(self.image_dir, clean_path)

                if os.path.exists(full_path):
                    self.data.append({
                        "image_path": full_path,
                        "class": class_name,
                        "class_idx": self.class_to_idx[class_name]
                    })
                    valid_images += 1

            except Exception:
                continue

        print(f"–í–∞–ª—ñ–¥–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å: {valid_images}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data[idx]

        try:
            image = Image.open(sample["image_path"]).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, sample["class_idx"]
        except Exception:
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –≤–∏–ø–∞–¥–∫–æ–≤–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            random_idx = random.randint(0, len(self.data) - 1)
            if random_idx != idx:
                return self.__getitem__(random_idx)
            else:
                dummy_img = torch.zeros(3, 224, 224)
                return dummy_img, sample["class_idx"]


def get_simple_transforms(image_size=224, is_training=True):
    """–ü—Ä–æ—Å—Ç—ñ —à–≤–∏–¥–∫—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó"""
    if is_training:
        return transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(15),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])
    else:
        return transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])


def get_fast_model(model_name, num_classes, dropout=0.3):
    """–®–≤–∏–¥–∫–∞ –º–æ–¥–µ–ª—å"""
    print(f"–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —à–≤–∏–¥–∫–æ—ó –º–æ–¥–µ–ª—ñ: {model_name}")

    if model_name == "mobilenet_v2":
        model = models.mobilenet_v2(pretrained=True)
        model.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(model.classifier[1].in_features, num_classes)
        )
    elif model_name == "resnet18":
        model = models.resnet18(pretrained=True)
        model.fc = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(model.fc.in_features, num_classes)
        )
    else:
        # Default –¥–æ –Ω–∞–π—à–≤–∏–¥—à–æ—ó –º–æ–¥–µ–ª—ñ
        model = models.mobilenet_v2(pretrained=True)
        model.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(model.classifier[1].in_features, num_classes)
        )

    return model


def create_fast_data_loaders(config):
    """–®–≤–∏–¥–∫—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á—ñ –¥–∞–Ω–∏—Ö"""
    print("–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —à–≤–∏–¥–∫–∏—Ö –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á—ñ–≤...")

    train_transform = get_simple_transforms(config["image_size"], is_training=True)
    val_transform = get_simple_transforms(config["image_size"], is_training=False)

    full_dataset = FastAgriculturalRiskDataset(
        csv_file=config["data_path"],
        image_dir=config["image_dir"],
        transform=None,
        risk_type=config["risk_type"]
    )

    if len(full_dataset) == 0:
        raise ValueError("–î–∞—Ç–∞—Å–µ—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π!")

    # –ê–Ω–∞–ª—ñ–∑ –∫–ª–∞—Å—ñ–≤
    class_counts = {}
    for sample in full_dataset.data:
        class_idx = sample["class_idx"]
        class_counts[class_idx] = class_counts.get(class_idx, 0) + 1

    # –ë–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ –∫–ª–∞—Å–∏ –∑ ‚â• 2 –∑—Ä–∞–∑–∫–∞–º–∏
    valid_class_indices = {class_idx for class_idx, count in class_counts.items() if count >= 2}
    valid_indices = []

    for i, sample in enumerate(full_dataset.data):
        if sample["class_idx"] in valid_class_indices:
            valid_indices.append(i)

    print(f"–ü—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó: {len(valid_indices)} –∑—Ä–∞–∑–∫—ñ–≤")
    print(f"–í–∞–ª—ñ–¥–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤: {len(valid_class_indices)}")

    if len(valid_indices) < 10:
        print("‚ö†Ô∏è –î–£–ñ–ï –ú–ê–õ–û –î–ê–ù–ò–•! –†–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –∑–±—ñ–ª—å—à–∏—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–æ–±—Ä–∞–∂–µ–Ω—å")

    # –í–ê–ñ–õ–ò–í–û: –ü–µ—Ä–µ–º–∞–ø–ø—ñ–Ω–≥ —ñ–Ω–¥–µ–∫—Å—ñ–≤ –∫–ª–∞—Å—ñ–≤
    # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–µ –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è: —Å—Ç–∞—Ä–∏–π_—ñ–Ω–¥–µ–∫—Å -> –Ω–æ–≤–∏–π_—ñ–Ω–¥–µ–∫—Å (0, 1, 2, ...)
    old_to_new_class_idx = {}
    new_class_names = []

    for new_idx, old_idx in enumerate(sorted(valid_class_indices)):
        old_to_new_class_idx[old_idx] = new_idx
        new_class_names.append(full_dataset.classes[old_idx])

    print(f"–ü–µ—Ä–µ–º–∞–ø–ø—ñ–Ω–≥ –∫–ª–∞—Å—ñ–≤: {len(old_to_new_class_idx)} –∫–ª–∞—Å—ñ–≤")

    # –û–Ω–æ–≤–ª—é—î–º–æ –¥–∞–Ω—ñ –∑ –Ω–æ–≤–∏–º–∏ —ñ–Ω–¥–µ–∫—Å–∞–º–∏
    updated_data = []
    for i in valid_indices:
        sample = full_dataset.data[i].copy()
        old_class_idx = sample["class_idx"]
        sample["class_idx"] = old_to_new_class_idx[old_class_idx]
        updated_data.append((i, sample))

    # –ü—Ä–æ—Å—Ç–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª
    train_data, val_data = train_test_split(
        updated_data,
        test_size=config["validation_split"],
        random_state=config["seed"]
    )

    train_indices = [item[0] for item in train_data]
    val_indices = [item[0] for item in val_data]

    # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç–∏ –∑ –æ–Ω–æ–≤–ª–µ–Ω–∏–º–∏ —ñ–Ω–¥–µ–∫—Å–∞–º–∏
    class RemappedDataset(torch.utils.data.Dataset):
        def __init__(self, base_dataset, indices, new_mapping, transform):
            self.base_dataset = base_dataset
            self.indices = indices
            self.new_mapping = new_mapping
            self.transform = transform

        def __len__(self):
            return len(self.indices)

        def __getitem__(self, idx):
            original_idx = self.indices[idx]
            sample = self.base_dataset.data[original_idx]

            try:
                image = Image.open(sample["image_path"]).convert('RGB')
                if self.transform:
                    image = self.transform(image)

                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–æ–≤–∏–π —ñ–Ω–¥–µ–∫—Å –∫–ª–∞—Å—É
                old_class_idx = sample["class_idx"]
                new_class_idx = self.new_mapping[old_class_idx]

                return image, new_class_idx
            except Exception as e:
                print(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {sample['image_path']}: {e}")
                dummy_img = torch.zeros(3, config["image_size"], config["image_size"])
                new_class_idx = self.new_mapping[sample["class_idx"]]
                return dummy_img, new_class_idx

    train_dataset = RemappedDataset(full_dataset, train_indices, old_to_new_class_idx, train_transform)
    val_dataset = RemappedDataset(full_dataset, val_indices, old_to_new_class_idx, val_transform)

    # –®–≤–∏–¥–∫—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á—ñ
    train_loader = DataLoader(
        train_dataset,
        batch_size=config["batch_size"],
        shuffle=True,
        num_workers=0,
        drop_last=False,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config["batch_size"],
        shuffle=False,
        num_workers=0,
    )

    num_classes = len(valid_class_indices)
    class_names = new_class_names

    print(f"–§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∞ –≤–∏–±—ñ—Ä–∫–∞: {len(train_dataset)} –∑—Ä–∞–∑–∫—ñ–≤")
    print(f"  –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∞ –≤–∏–±—ñ—Ä–∫–∞: {len(val_dataset)} –∑—Ä–∞–∑–∫—ñ–≤")
    print(f"  –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤: {num_classes}")
    print(f"  –ë–∞—Ç—á—ñ–≤ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è: {len(train_loader)}")
    print(f"  –ë–∞—Ç—á—ñ–≤ –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó: {len(val_loader)}")
    print(f"  –ö–ª–∞—Å–∏: {class_names[:5]}..." if len(class_names) > 5 else f"  –ö–ª–∞—Å–∏: {class_names}")

    return train_loader, val_loader, num_classes, class_names


def freeze_backbone(model, model_name):
    """–ó–∞–º–æ—Ä–æ–∂—É—î backbone"""
    if model_name == "mobilenet_v2":
        for param in model.features.parameters():
            param.requires_grad = False
        print("üßä Backbone –∑–∞–º–æ—Ä–æ–∂–µ–Ω–æ")
    elif model_name == "resnet18":
        for name, param in model.named_parameters():
            if 'fc' not in name:
                param.requires_grad = False
        print("üßä Backbone –∑–∞–º–æ—Ä–æ–∂–µ–Ω–æ")


def unfreeze_backbone(model):
    """–†–æ–∑–º–æ—Ä–æ–∂—É—î –≤—Å—é –º–æ–¥–µ–ª—å"""
    for param in model.parameters():
        param.requires_grad = True
    print("üî• –ú–æ–¥–µ–ª—å —Ä–æ–∑–º–æ—Ä–æ–∂–µ–Ω–∞")


def train_one_epoch_fast(model, train_loader, criterion, optimizer, device, epoch):
    """–®–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è –±–µ–∑ –∑–∞–π–≤–∏—Ö –∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ–π"""
    model.train()
    running_loss = 0.0
    correct_preds = 0
    total_samples = 0

    progress_bar = tqdm(train_loader, desc=f"–ï–ø–æ—Ö–∞ {epoch}")

    for inputs, labels in progress_bar:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        running_loss += loss.item() * inputs.size(0)
        _, predicted = torch.max(outputs, 1)
        correct_preds += (predicted == labels).sum().item()
        total_samples += labels.size(0)

        progress_bar.set_postfix({
            "loss": loss.item(),
            "acc": correct_preds / total_samples,
            "lr": optimizer.param_groups[0]['lr']
        })

    epoch_loss = running_loss / total_samples
    epoch_acc = correct_preds / total_samples

    return epoch_loss, epoch_acc


def validate_fast(model, val_loader, criterion, device):
    """–®–≤–∏–¥–∫–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –±–µ–∑ TTA"""
    model.eval()
    running_loss = 0.0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in tqdm(val_loader, desc="–í–∞–ª—ñ–¥–∞—Ü—ñ—è", leave=False):
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item() * inputs.size(0)

            _, predicted = torch.max(outputs, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    val_loss = running_loss / len(val_loader.dataset)
    val_acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='weighted') if len(set(all_labels)) > 1 else 0.0

    return val_loss, val_acc, f1, all_preds, all_labels


def main():
    print("‚ö° –®–í–ò–î–ö–ï –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è –º–∞–ª–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤")

    # W&B —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
    wandb.init(project="agri-risk-classification-fast", config=CONFIG)
    config = wandb.config

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"–ü—Ä–∏—Å—Ç—Ä—ñ–π: {device}")

    # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∞–Ω—ñ
    train_loader, val_loader, num_classes, class_names = create_fast_data_loaders(config)

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –º—ñ–Ω—ñ–º–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö
    if len(train_loader) < 2:
        print("‚ùå –ö–†–ò–¢–ò–ß–ù–û –ú–ê–õ–û –î–ê–ù–ò–•!")
        print("–°–ø—Ä–æ–±—É–π—Ç–µ –∑–º–µ–Ω—à–∏—Ç–∏ batch_size –∞–±–æ –∑–±—ñ–ª—å—à–∏—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–æ–±—Ä–∞–∂–µ–Ω—å")
        return

    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å
    model = get_fast_model(config.model_name, num_classes, config.dropout)
    model = model.to(device)

    # –ó–∞–º–æ—Ä–æ–∂—É—î–º–æ backbone —Å–ø–æ—á–∞—Ç–∫—É
    freeze_backbone(model, config.model_name)

    # –ü—Ä–æ—Å—Ç–∞ —Ñ—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç
    criterion = nn.CrossEntropyLoss(label_smoothing=config["label_smoothing"])

    # –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä
    optimizer = optim.Adam(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )

    # –ü—Ä–æ—Å—Ç–∏–π –ø–ª–∞–Ω—É–≤–∞–ª—å–Ω–∏–∫
    scheduler = ReduceLROnPlateau(
        optimizer,
        mode='max',
        factor=0.5,
        patience=4,
        verbose=True,
        min_lr=1e-6
    )

    best_val_f1 = 0.0
    patience_counter = 0

    os.makedirs("models", exist_ok=True)

    print(f"üöÄ –ü–æ—á–∏–Ω–∞—î–º–æ —à–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è...")

    # –ù–∞–≤—á–∞–Ω–Ω—è
    for epoch in range(config.num_epochs):
        print(f"\n–ï–ø–æ—Ö–∞ {epoch + 1}/{config.num_epochs}")

        # –†–æ–∑–º–æ—Ä–æ–∂—É—î–º–æ –ø—ñ—Å–ª—è –∫—ñ–ª—å–∫–æ—Ö –µ–ø–æ—Ö
        if epoch == config["freeze_epochs"]:
            unfreeze_backbone(model)

        # –ù–∞–≤—á–∞–Ω–Ω—è
        train_loss, train_acc = train_one_epoch_fast(
            model, train_loader, criterion, optimizer, device, epoch + 1
        )

        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è
        val_loss, val_acc, val_f1, all_preds, all_labels = validate_fast(
            model, val_loader, criterion, device
        )

        # –û–Ω–æ–≤–ª—é—î–º–æ –ø–ª–∞–Ω—É–≤–∞–ª—å–Ω–∏–∫
        scheduler.step(val_f1)

        # –õ–æ–≥—É–≤–∞–Ω–Ω—è
        wandb.log({
            "epoch": epoch,
            "train_loss": train_loss,
            "train_acc": train_acc,
            "val_loss": val_loss,
            "val_acc": val_acc,
            "val_f1": val_f1,
            "lr": optimizer.param_groups[0]['lr']
        })

        print(f"Train: Loss={train_loss:.3f}, Acc={train_acc:.3f}")
        print(f"Val: Loss={val_loss:.3f}, Acc={val_acc:.3f}, F1={val_f1:.3f}")

        # Early stopping
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            patience_counter = 0
            torch.save(model.state_dict(), f"models/{config.risk_type}_{config.model_name}_best_fast.pt")
            print(f"‚úÖ –ù–æ–≤–∞ –Ω–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å! F1: {best_val_f1:.3f}")
        else:
            patience_counter += 1

        if patience_counter >= config.patience:
            print(f"‚èπÔ∏è Early stopping –ø—ñ—Å–ª—è {epoch + 1} –µ–ø–æ—Ö")
            break

    print(f"\nüéâ –®–≤–∏–¥–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"–ù–∞–π–∫—Ä–∞—â–∏–π F1: {best_val_f1:.3f}")

    # –û—Ü—ñ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
    if best_val_f1 >= 0.60:
        print("üéØ –î–û–ë–†–ï! –ú–æ–¥–µ–ª—å –Ω–∞–≤—á–∏–ª–∞—Å—è")
        print("üí° –î–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Å–ø—Ä–æ–±—É–π—Ç–µ:")
        print("  - –ó–±—ñ–ª—å—à–∏—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö")
        print("  - –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –±—ñ–ª—å—à—ñ –º–æ–¥–µ–ª—ñ")
        print("  - –î–æ–¥–∞—Ç–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó")
    elif best_val_f1 >= 0.30:
        print("üìà –Ñ –ø—Ä–æ–≥—Ä–µ—Å, –∞–ª–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –±—ñ–ª—å—à–µ")
        print("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:")
        print("  - –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —è–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö")
        print("  - –ó–±—ñ–ª—å—à—ñ—Ç—å learning rate")
        print("  - –ó–º–µ–Ω—à—ñ—Ç—å –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤")
    else:
        print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –ø–æ–≥–∞–Ω–æ –Ω–∞–≤—á–∞—î—Ç—å—Å—è")
        print("üí° –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ:")
        print("  - –ü—Ä–∞–≤–∏–ª—å–Ω—ñ—Å—Ç—å –º—ñ—Ç–æ–∫")
        print("  - –Ø–∫—ñ—Å—Ç—å –∑–æ–±—Ä–∞–∂–µ–Ω—å")
        print("  - –†–æ–∑–º—ñ—Ä –¥–∞—Ç–∞—Å–µ—Ç—É")

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
    metadata = {
        "model_name": config.model_name,
        "num_classes": num_classes,
        "class_names": class_names,
        "risk_type": config.risk_type,
        "best_val_f1": best_val_f1,
        "config": dict(config),
        "dataset_size": {
            "train": len(train_loader.dataset),
            "val": len(val_loader.dataset),
            "total": len(train_loader.dataset) + len(val_loader.dataset)
        }
    }

    with open(f"models/{config.risk_type}_{config.model_name}_metadata_fast.json", "w", encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    wandb.finish()


if __name__ == "__main__":
    main()