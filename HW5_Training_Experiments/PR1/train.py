#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
import pandas as pd
import numpy as np
import wandb
import json
from PIL import Image, ImageFilter
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
from tqdm import tqdm
import random

# –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
CONFIG = {
    "data_path": "../crawler/downloads/",
    "image_dir": "../crawler/downloads/images/",
    "model_name": "mobilenet_v2",  # –ú–µ–Ω—à–∞ –º–æ–¥–µ–ª—å
    "num_epochs": 50,
    "batch_size": 16,  # –ú–µ–Ω—à–∏–π batch
    "learning_rate": 0.0001,  # –ú–µ–Ω—à–∏–π LR
    "weight_decay": 1e-3,  # –°–∏–ª—å–Ω—ñ—à–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
    "dropout": 0.5,
    "seed": 42,
    "validation_split": 0.25,
    "risk_type": "diseases",
    "patience": 10,  # Early stopping
    "freeze_backbone_epochs": 5,
    "label_smoothing": 0.1,
}

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è seed
torch.manual_seed(CONFIG["seed"])
np.random.seed(CONFIG["seed"])
random.seed(CONFIG["seed"])


class ImprovedAgriculturalRiskDataset(Dataset):
    def __init__(self, csv_file, image_dir, transform=None, risk_type="diseases"):
        """–ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∑ –∫—Ä–∞—â–æ—é –æ–±—Ä–æ–±–∫–æ—é –ø–æ–º–∏–ª–æ–∫"""
        self.image_dir = image_dir
        self.transform = transform
        self.risk_type = risk_type

        print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è —Ç–∏–ø—É —Ä–∏–∑–∏–∫—É: {risk_type}")

        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ
        try:
            if risk_type == "diseases":
                self.risks_df = pd.read_csv(os.path.join(csv_file, "diseases.csv"))
                self.images_df = pd.read_csv(os.path.join(csv_file, "disease_images.csv"))
            # –î–æ–¥–∞–π—Ç–µ —ñ–Ω—à—ñ —Ç–∏–ø–∏ —Ä–∏–∑–∏–∫—ñ–≤ –∑–∞ –ø–æ—Ç—Ä–µ–±–∏

            print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —Ä–∏–∑–∏–∫—ñ–≤: {len(self.risks_df)}")
            print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑–æ–±—Ä–∞–∂–µ–Ω—å: {len(self.images_df)}")

        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è CSV: {e}")
            self.data = []
            self.classes = []
            self.class_to_idx = {}
            return

        # –°—Ç–≤–æ—Ä—é—î–º–æ –º–∞–ø–ø—ñ–Ω–≥ –∫–ª–∞—Å—ñ–≤
        self.classes = self.risks_df["name"].unique()
        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}
        print(f"–ó–Ω–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—ñ–≤: {len(self.classes)}")

        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–æ—é –æ–±—Ä–æ–±–∫–æ—é —à–ª—è—Ö—ñ–≤
        self.data = []
        risk_id_field = "disease_id"

        valid_images = 0
        for _, row in self.images_df.iterrows():
            try:
                risk_id = row[risk_id_field]
                risk_matches = self.risks_df[self.risks_df["id"] == risk_id]

                if risk_matches.empty:
                    continue

                risk_row = risk_matches.iloc[0]
                class_name = risk_row["name"]
                image_path = row["image_path"]

                # –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –æ–±—Ä–æ–±–∫–∞ —à–ª—è—Ö—ñ–≤
                clean_path = image_path.replace("downloads\\images/", "").replace("downloads/images/", "")
                clean_path = clean_path.replace("downloads\\", "").replace("downloads/", "")

                possible_paths = [
                    os.path.join(self.image_dir, clean_path),
                    os.path.join(self.image_dir, os.path.basename(image_path)),
                    image_path,
                ]

                found_path = None
                for path in possible_paths:
                    if os.path.exists(path):
                        found_path = path
                        break

                if found_path:
                    self.data.append({
                        "image_path": found_path,
                        "class": class_name,
                        "class_idx": self.class_to_idx[class_name]
                    })
                    valid_images += 1

            except Exception as e:
                continue

        print(f"–í–∞–ª—ñ–¥–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å: {valid_images}")

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∑—Ä–∞–∑–∫—ñ–≤
        if len(self.data) < 10:
            raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è!")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data[idx]
        image_path = sample["image_path"]

        try:
            image = Image.open(image_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, sample["class_idx"]
        except Exception as e:
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –≤–∏–ø–∞–¥–∫–æ–≤–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑ –¥–∞—Ç–∞—Å–µ—Ç—É
            random_idx = random.randint(0, len(self.data) - 1)
            if random_idx != idx:
                return self.__getitem__(random_idx)
            else:
                dummy_img = torch.zeros(3, 224, 224)
                return dummy_img, sample["class_idx"]


def get_strong_transforms():
    """–°–∏–ª—å–Ω—ñ –∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó –¥–ª—è –±–æ—Ä–æ—Ç—å–±–∏ –∑ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è–º"""
    return transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.3),
        transforms.RandomRotation(30),
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
        transforms.RandomApply([transforms.GaussianBlur(3)], p=0.3),
        transforms.RandomApply([transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))], p=0.3),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        transforms.RandomErasing(p=0.3, scale=(0.02, 0.1)),
    ])


def get_val_transforms():
    """–í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó"""
    return transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])


def create_improved_data_loaders(config):
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–∏—Ö –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á—ñ–≤ –¥–∞–Ω–∏—Ö"""
    print("–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–∏—Ö –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á—ñ–≤ –¥–∞–Ω–∏—Ö...")

    train_transform = get_strong_transforms()
    val_transform = get_val_transforms()

    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ–≤–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç
    full_dataset = ImprovedAgriculturalRiskDataset(
        csv_file=config["data_path"],
        image_dir=config["image_dir"],
        transform=None,
        risk_type=config["risk_type"]
    )

    if len(full_dataset) == 0:
        raise ValueError("–î–∞—Ç–∞—Å–µ—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π!")

    print(f"–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑—Ä–∞–∑–∫—ñ–≤: {len(full_dataset)}")

    # –ê–Ω–∞–ª—ñ–∑ —Ä–æ–∑–ø–æ–¥—ñ–ª—É –∫–ª–∞—Å—ñ–≤
    class_counts = {}
    for sample in full_dataset.data:
        class_idx = sample["class_idx"]
        class_counts[class_idx] = class_counts.get(class_idx, 0) + 1

    print(f"–†–æ–∑–ø–æ–¥—ñ–ª –ø–æ –∫–ª–∞—Å–∞—Ö (—Ç–æ–ø-10):")
    sorted_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)
    for i, (class_idx, count) in enumerate(sorted_classes[:10]):
        class_name = full_dataset.classes[class_idx]
        print(f"  {class_name}: {count} –∑—Ä–∞–∑–∫—ñ–≤")

    # –í–∏–¥–∞–ª—è—î–º–æ –∫–ª–∞—Å–∏ –∑ –∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∑—Ä–∞–∑–∫—ñ–≤
    min_samples = 3
    valid_indices = []
    for i, sample in enumerate(full_dataset.data):
        if class_counts[sample["class_idx"]] >= min_samples:
            valid_indices.append(i)

    print(f"–ü—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó: {len(valid_indices)} –∑—Ä–∞–∑–∫—ñ–≤")

    # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—É —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—É –≤–∏–±—ñ—Ä–∫–∏
    try:
        train_indices, val_indices = train_test_split(
            valid_indices,
            test_size=config["validation_split"],
            random_state=config["seed"],
            stratify=[full_dataset.data[i]["class_idx"] for i in valid_indices]
        )
    except ValueError:
        # –Ø–∫—â–æ —Å—Ç—Ä–∞—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è –Ω–µ–º–æ–∂–ª–∏–≤–∞
        train_indices, val_indices = train_test_split(
            valid_indices,
            test_size=config["validation_split"],
            random_state=config["seed"]
        )

    # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç–∏
    train_dataset = torch.utils.data.Subset(
        ImprovedAgriculturalRiskDataset(
            csv_file=config["data_path"],
            image_dir=config["image_dir"],
            transform=train_transform,
            risk_type=config["risk_type"]
        ),
        train_indices
    )

    val_dataset = torch.utils.data.Subset(
        ImprovedAgriculturalRiskDataset(
            csv_file=config["data_path"],
            image_dir=config["image_dir"],
            transform=val_transform,
            risk_type=config["risk_type"]
        ),
        val_indices
    )

    # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á—ñ
    train_loader = DataLoader(
        train_dataset,
        batch_size=config["batch_size"],
        shuffle=True,
        num_workers=2,
        drop_last=True,  # –î–æ–ø–æ–º–∞–≥–∞—î –∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config["batch_size"],
        shuffle=False,
        num_workers=2,
    )

    # –û–Ω–æ–≤–ª—é—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤
    unique_classes = set(full_dataset.data[i]["class_idx"] for i in valid_indices)
    num_classes = len(unique_classes)
    class_names = [full_dataset.classes[i] for i in sorted(unique_classes)]

    print(f"–§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∞ –≤–∏–±—ñ—Ä–∫–∞: {len(train_dataset)} –∑—Ä–∞–∑–∫—ñ–≤")
    print(f"  –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∞ –≤–∏–±—ñ—Ä–∫–∞: {len(val_dataset)} –∑—Ä–∞–∑–∫—ñ–≤")
    print(f"  –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤: {num_classes}")

    return train_loader, val_loader, num_classes, class_names


def get_improved_model(model_name, num_classes, dropout=0.5):
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ –∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é"""
    print(f"–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ {model_name}")

    if model_name == "mobilenet_v2":
        model = models.mobilenet_v2(pretrained=True)
        # –ó–∞–º—ñ–Ω—é—î–º–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä –∑ dropout
        model.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(model.classifier[1].in_features, num_classes)
        )
    elif model_name == "resnet18":
        model = models.resnet18(pretrained=True)
        # –î–æ–¥–∞—î–º–æ dropout –ø–µ—Ä–µ–¥ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–º —à–∞—Ä–æ–º
        model.fc = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(model.fc.in_features, num_classes)
        )
    else:
        raise ValueError(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: {model_name}")

    return model


def freeze_backbone(model, model_name):
    """–ó–∞–º–æ—Ä–æ–∂—É—î backbone –º–æ–¥–µ–ª—ñ"""
    if model_name == "mobilenet_v2":
        for param in model.features.parameters():
            param.requires_grad = False
    elif model_name == "resnet18":
        for param in model.parameters():
            param.requires_grad = False
        # –†–æ–∑–º–æ—Ä–æ–∂—É—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π —à–∞—Ä
        for param in model.fc.parameters():
            param.requires_grad = True


def unfreeze_backbone(model):
    """–†–æ–∑–º–æ—Ä–æ–∂—É—î –≤—Å—é –º–æ–¥–µ–ª—å"""
    for param in model.parameters():
        param.requires_grad = True


def train_one_epoch_improved(model, train_loader, criterion, optimizer, device, epoch):
    """–ü–æ–∫—Ä–∞—â–µ–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –¥–æ–¥–∞—Ç–∫–æ–≤–æ—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é"""
    model.train()
    running_loss = 0.0
    correct_preds = 0
    total_samples = 0

    progress_bar = tqdm(train_loader, desc=f"–ù–∞–≤—á–∞–Ω–Ω—è –µ–ø–æ—Ö–∞ {epoch}")

    for batch_idx, (inputs, labels) in enumerate(progress_bar):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, labels)

        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        running_loss += loss.item() * inputs.size(0)
        _, predicted = torch.max(outputs, 1)
        correct_preds += (predicted == labels).sum().item()
        total_samples += labels.size(0)

        progress_bar.set_postfix({
            "loss": loss.item(),
            "acc": correct_preds / total_samples,
            "lr": optimizer.param_groups[0]['lr']
        })

    epoch_loss = running_loss / total_samples
    epoch_acc = correct_preds / total_samples

    return epoch_loss, epoch_acc


def validate_improved(model, val_loader, criterion, device):
    """–ü–æ–∫—Ä–∞—â–µ–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è"""
    model.eval()
    running_loss = 0.0
    correct_preds = 0
    total_samples = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in tqdm(val_loader, desc="–í–∞–ª—ñ–¥–∞—Ü—ñ—è"):
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs, 1)
            correct_preds += (predicted == labels).sum().item()
            total_samples += labels.size(0)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    val_loss = running_loss / total_samples
    val_acc = correct_preds / total_samples
    f1 = f1_score(all_labels, all_preds, average='weighted')

    return val_loss, val_acc, f1, all_preds, all_labels


def main():
    print("–ü–æ—á–∞—Ç–æ–∫ –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é")

    # W&B —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
    wandb.init(project="agri-risk-classification-improved", config=CONFIG)
    config = wandb.config

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—Ä–∏—Å—Ç—Ä—ñ–π: {device}")

    # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∞–Ω—ñ
    train_loader, val_loader, num_classes, class_names = create_improved_data_loaders(config)

    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å
    model = get_improved_model(config.model_name, num_classes, config.dropout)
    model = model.to(device)

    # –ó–∞–º–æ—Ä–æ–∂—É—î–º–æ backbone —Å–ø–æ—á–∞—Ç–∫—É
    if config.freeze_backbone_epochs > 0:
        freeze_backbone(model, config.model_name)
        print(f"Backbone –∑–∞–º–æ—Ä–æ–∂–µ–Ω–æ –Ω–∞ –ø–µ—Ä—à—ñ {config.freeze_backbone_epochs} –µ–ø–æ—Ö")

    # –§—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç –∑ label smoothing
    criterion = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)

    # –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä
    optimizer = optim.Adam(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )

    # –ü–ª–∞–Ω—É–≤–∞–ª—å–Ω–∏–∫ learning rate
    scheduler = ReduceLROnPlateau(
        optimizer,
        mode='max',
        factor=0.5,
        patience=5,
        verbose=True,
        min_lr=1e-7
    )

    # Early stopping
    best_val_f1 = 0.0
    patience_counter = 0

    os.makedirs("models", exist_ok=True)

    # –ù–∞–≤—á–∞–Ω–Ω—è
    for epoch in range(config.num_epochs):
        print(f"–ï–ø–æ—Ö–∞ {epoch + 1}/{config.num_epochs}")

        # –†–æ–∑–º–æ—Ä–æ–∂—É—î–º–æ backbone –ø—ñ—Å–ª—è –ø–µ–≤–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –µ–ø–æ—Ö
        if epoch == config.freeze_backbone_epochs:
            unfreeze_backbone(model)
            print("Backbone —Ä–æ–∑–º–æ—Ä–æ–∂–µ–Ω–æ!")

        # –ù–∞–≤—á–∞–Ω–Ω—è
        train_loss, train_acc = train_one_epoch_improved(
            model, train_loader, criterion, optimizer, device, epoch + 1
        )

        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è
        val_loss, val_acc, val_f1, all_preds, all_labels = validate_improved(
            model, val_loader, criterion, device
        )

        # –û–Ω–æ–≤–ª—é—î–º–æ learning rate
        scheduler.step(val_f1)

        # –õ–æ–≥—É–≤–∞–Ω–Ω—è
        wandb.log({
            "epoch": epoch,
            "train_loss": train_loss,
            "train_acc": train_acc,
            "val_loss": val_loss,
            "val_acc": val_acc,
            "val_f1": val_f1,
            "lr": optimizer.param_groups[0]['lr']
        })

        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}")
        print(f"LR: {optimizer.param_groups[0]['lr']:.2e}")

        # Early stopping —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–æ—ó –º–æ–¥–µ–ª—ñ
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            patience_counter = 0
            torch.save(model.state_dict(), f"models/{config.risk_type}_{config.model_name}_best_improved.pt")
            print(f"‚úÖ –ù–æ–≤–∞ –Ω–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å! F1: {best_val_f1:.4f}")
        else:
            patience_counter += 1

        if patience_counter >= config.patience:
            print(f"Early stopping –ø—ñ—Å–ª—è {epoch + 1} –µ–ø–æ—Ö")
            break

        print("-" * 60)

    # –§—ñ–Ω–∞–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    print(f"\nüéâ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"–ù–∞–π–∫—Ä–∞—â–∏–π –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏–π F1: {best_val_f1:.4f}")

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
    metadata = {
        "model_name": config.model_name,
        "num_classes": num_classes,
        "class_names": class_names,
        "risk_type": config.risk_type,
        "best_val_f1": best_val_f1,
        "config": dict(config)
    }

    with open(f"models/{config.risk_type}_{config.model_name}_metadata_improved.json", "w", encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    wandb.finish()


if __name__ == "__main__":
    main()