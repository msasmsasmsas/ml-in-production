# PR6: Distributed Training with PyTorch, Accelerate, and Ray

This directory contains code for distributed training of a DistilBERT model using PyTorch, Accelerate, and Ray.

## Files
- `distributed_train.py`: Script for distributed training.
- `requirements.txt`: Dependencies.

## Usage
1. Install dependencies: `pip install -r requirements.txt`.
2. Run training: `python distributed_train.py`.

## Notes
Ensure a multi-GPU setup or a cluster is available for distributed training.