#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
import wandb
import json
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
from tqdm import tqdm
import argparse
import random

# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è sweep –¥–ª—è –ø–æ—à—É–∫—É –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ (–í–ò–ü–†–ê–í–õ–ï–ù–ê)
SWEEP_CONFIG = {
    "method": "bayes",  # random, grid, bayes
    "metric": {
        "name": "val_f1",
        "goal": "maximize"
    },
    "parameters": {
        "model_name": {
            "values": ["resnet18", "resnet34", "mobilenet_v2"]
        },
        "batch_size": {
            "values": [8, 16, 32]
        },
        # –í–ò–ü–†–ê–í–õ–ï–ù–û: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ log_uniform_values –∑–∞–º—ñ—Å—Ç—å log_uniform
        "learning_rate": {
            "min": 0.0001,  # 1e-4
            "max": 0.005,  # 5e-3
            "distribution": "log_uniform_values"
        },
        "weight_decay": {
            "min": 0.00001,  # 1e-5
            "max": 0.01,  # 1e-2
            "distribution": "log_uniform_values"
        },
        "dropout": {
            "min": 0.1,
            "max": 0.6,
            "distribution": "uniform"
        },
        "num_epochs": {
            "values": [15, 20, 25]
        },
        "validation_split": {
            "values": [0.2, 0.25, 0.3]
        }
    }
}


class OptimizedDataset(Dataset):
    def __init__(self, csv_file, image_dir, transform=None, risk_type="diseases"):
        """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∑ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è–º —à–ª—è—Ö—ñ–≤"""
        self.image_dir = image_dir
        self.transform = transform

        print(f"üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {risk_type}")

        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ CSV —Ñ–∞–π–ª–∏
        if risk_type == "diseases":
            self.risks_df = pd.read_csv(os.path.join(csv_file, "diseases.csv"))
            self.images_df = pd.read_csv(os.path.join(csv_file, "disease_images.csv"))
            risk_id_field = "disease_id"
        elif risk_type == "pests":
            self.risks_df = pd.read_csv(os.path.join(csv_file, "vermins.csv"))
            self.images_df = pd.read_csv(os.path.join(csv_file, "vermin_images.csv"))
            risk_id_field = "vermin_id"
        elif risk_type == "weeds":
            self.risks_df = pd.read_csv(os.path.join(csv_file, "weeds.csv"))
            self.images_df = pd.read_csv(os.path.join(csv_file, "weed_images.csv"))
            risk_id_field = "weed_id"

        print(f"üìä –†–∏–∑–∏–∫—ñ–≤: {len(self.risks_df)}, –ó–æ–±—Ä–∞–∂–µ–Ω—å: {len(self.images_df)}")

        # –ó–±–∏—Ä–∞—î–º–æ –¥–∞–Ω—ñ –∑ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è–º —à–ª—è—Ö—ñ–≤
        self.data = []
        self.class_names = []

        for _, row in self.images_df.iterrows():
            try:
                risk_id = row[risk_id_field]
                risk_info = self.risks_df[self.risks_df["id"] == risk_id]

                if risk_info.empty:
                    continue

                class_name = risk_info.iloc[0]["name"]
                image_path = row["image_path"]

                # –í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è —à–ª—è—Ö—ñ–≤ (—è–∫ —É —Ä–æ–±–æ—á—ñ–π –≤–µ—Ä—Å—ñ—ó)
                clean_path = image_path.replace("downloads\\images/", "").replace("downloads/images/", "")
                clean_path = clean_path.replace("downloads\\", "").replace("downloads/", "")
                full_path = os.path.join(self.image_dir, clean_path)

                if os.path.exists(full_path):
                    self.data.append({
                        "image_path": full_path,
                        "class_name": class_name
                    })

                    if class_name not in self.class_names:
                        self.class_names.append(class_name)

            except Exception as e:
                continue

        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–∞–ø–ø—ñ–Ω–≥—É –∫–ª–∞—Å—ñ–≤
        self.class_names = sorted(list(set(self.class_names)))
        self.class_to_idx = {name: idx for idx, name in enumerate(self.class_names)}

        # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∫–ª–∞—Å—ñ–≤ –∑ –º–∞–ª–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∑—Ä–∞–∑–∫—ñ–≤
        class_counts = {}
        for item in self.data:
            class_name = item["class_name"]
            class_counts[class_name] = class_counts.get(class_name, 0) + 1

        min_samples = 2
        filtered_data = []
        valid_classes = {name for name, count in class_counts.items() if count >= min_samples}

        for item in self.data:
            if item["class_name"] in valid_classes:
                item["class_idx"] = self.class_to_idx[item["class_name"]]
                filtered_data.append(item)

        self.data = filtered_data
        self.class_names = sorted(list(valid_classes))
        self.class_to_idx = {name: idx for idx, name in enumerate(self.class_names)}

        # –û–Ω–æ–≤–ª—é—î–º–æ —ñ–Ω–¥–µ–∫—Å–∏
        for item in self.data:
            item["class_idx"] = self.class_to_idx[item["class_name"]]

        print(f"‚úÖ –ü—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó: {len(self.data)} –∑–æ–±—Ä–∞–∂–µ–Ω—å, {len(self.class_names)} –∫–ª–∞—Å—ñ–≤")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        try:
            image = Image.open(item["image_path"]).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, item["class_idx"]
        except Exception as e:
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –≤–∏–ø–∞–¥–∫–æ–≤–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ
            random_idx = random.randint(0, len(self.data) - 1)
            if random_idx != idx:
                return self.__getitem__(random_idx)
            else:
                dummy_img = torch.zeros(3, 224, 224)
                return dummy_img, item["class_idx"]


def get_transforms(config):
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ–π –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó"""

    # –ë–∞–∑–æ–≤—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(0.5),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    return train_transform, val_transform


def create_optimized_loaders(config):
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏—Ö –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á—ñ–≤ –¥–∞–Ω–∏—Ö"""
    print("üîÑ –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á—ñ–≤ –¥–∞–Ω–∏—Ö...")

    train_transform, val_transform = get_transforms(config)

    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ–≤–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç
    dataset = OptimizedDataset(
        csv_file=config["data_path"],
        image_dir=config["image_dir"],
        transform=None,
        risk_type=config.get("risk_type", "diseases")
    )

    if len(dataset.data) < 10:
        raise ValueError(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö: {len(dataset.data)} –∑–æ–±—Ä–∞–∂–µ–Ω—å")

    # –†–æ–∑–¥—ñ–ª—è—î–º–æ –¥–∞–Ω—ñ
    train_data, val_data = train_test_split(
        dataset.data,
        test_size=config.get("validation_split", 0.25),
        random_state=config.get("seed", 42)
    )

    print(f"üìä –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è: {len(train_data)} —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö, {len(val_data)} –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏—Ö")

    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∏
    class OptimizedSubset(Dataset):
        def __init__(self, data, class_to_idx, transform):
            self.data = data
            self.class_to_idx = class_to_idx
            self.transform = transform

        def __len__(self):
            return len(self.data)

        def __getitem__(self, idx):
            item = self.data[idx]
            try:
                image = Image.open(item["image_path"]).convert('RGB')
                if self.transform:
                    image = self.transform(image)
                return image, item["class_idx"]
            except:
                dummy_img = torch.zeros(3, 224, 224)
                return dummy_img, item["class_idx"]

    train_dataset = OptimizedSubset(train_data, dataset.class_to_idx, train_transform)
    val_dataset = OptimizedSubset(val_data, dataset.class_to_idx, val_transform)

    # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á—ñ
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.get("batch_size", 16),
        shuffle=True,
        num_workers=0,  # –î–ª—è CPU
        drop_last=False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config.get("batch_size", 16),
        shuffle=False,
        num_workers=0
    )

    num_classes = len(dataset.class_names)

    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –ö–ª–∞—Å—ñ–≤: {num_classes}, –ë–∞—Ç—á—ñ–≤: train={len(train_loader)}, val={len(val_loader)}")

    return train_loader, val_loader, num_classes, dataset.class_names


def get_optimized_model(model_name, num_classes, dropout=0.3):
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º dropout"""
    print(f"üß† –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ {model_name} –¥–ª—è {num_classes} –∫–ª–∞—Å—ñ–≤")

    if model_name == "resnet18":
        model = models.resnet18(weights='IMAGENET1K_V1')
        model.fc = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(model.fc.in_features, num_classes)
        )
    elif model_name == "resnet34":
        model = models.resnet34(weights='IMAGENET1K_V1')
        model.fc = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(model.fc.in_features, num_classes)
        )
    elif model_name == "mobilenet_v2":
        model = models.mobilenet_v2(weights='IMAGENET1K_V1')
        model.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(model.classifier[1].in_features, num_classes)
        )
    else:
        # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º MobileNetV2
        print(f"‚ö†Ô∏è –ù–µ–≤—ñ–¥–æ–º–∞ –º–æ–¥–µ–ª—å {model_name}, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ mobilenet_v2")
        model = models.mobilenet_v2(weights='IMAGENET1K_V1')
        model.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(model.classifier[1].in_features, num_classes)
        )

    return model


def train_epoch_optimized(model, loader, criterion, optimizer, device):
    """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –æ–¥–Ω—ñ—î—ó –µ–ø–æ—Ö–∏"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for inputs, labels in tqdm(loader, desc="–ù–∞–≤—á–∞–Ω–Ω—è", leave=False):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    return total_loss / len(loader), correct / total


def validate_optimized(model, loader, criterion, device):
    """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in tqdm(loader, desc="–í–∞–ª—ñ–¥–∞—Ü—ñ—è", leave=False):
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    val_loss = total_loss / len(loader)
    val_acc = accuracy_score(all_labels, all_preds)
    val_f1 = f1_score(all_labels, all_preds, average='weighted')

    return val_loss, val_acc, val_f1


def train_model_sweep():
    """–§—É–Ω–∫—Ü—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è sweep"""

    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è W&B –∑ –ü–†–ê–í–ò–õ–¨–ù–ò–ú –ø—Ä–æ–µ–∫—Ç–æ–º
    with wandb.init(project="agri-risk-hyperparameter-search") as run:
        config = wandb.config

        # –î–æ–¥–∞—î–º–æ –±–∞–∑–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        config_dict = dict(config)
        config_dict.update({
            "data_path": "../crawler/downloads/",
            "image_dir": "../crawler/downloads/images/",
            "risk_type": "diseases",
            "seed": 42
        })

        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è seed
        torch.manual_seed(config_dict["seed"])
        np.random.seed(config_dict["seed"])
        random.seed(config_dict["seed"])

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üíª –ü—Ä–∏—Å—Ç—Ä—ñ–π: {device}")

        try:
            # –î–∞–Ω—ñ
            train_loader, val_loader, num_classes, class_names = create_optimized_loaders(config_dict)

            if num_classes < 2:
                print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –∫–ª–∞—Å—ñ–≤: {num_classes}")
                wandb.log({"val_f1": 0.0})
                return

            # –ú–æ–¥–µ–ª—å
            model = get_optimized_model(
                config.model_name,
                num_classes,
                config.get("dropout", 0.3)
            )
            model = model.to(device)

            # –ó–∞–º–æ—Ä–æ–∂—É–≤–∞–Ω–Ω—è backbone –Ω–∞ –ø–µ—Ä—à—ñ –µ–ø–æ—Ö–∏
            freeze_epochs = 3
            if hasattr(model, 'features'):  # MobileNet
                for param in model.features.parameters():
                    param.requires_grad = False
            else:  # ResNet
                for name, param in model.named_parameters():
                    if 'fc' not in name:
                        param.requires_grad = False

            print("üßä Backbone –∑–∞–º–æ—Ä–æ–∂–µ–Ω–æ")

            # –ù–∞–≤—á–∞–Ω–Ω—è
            criterion = nn.CrossEntropyLoss()
            optimizer = optim.Adam(
                model.parameters(),
                lr=config.learning_rate,
                weight_decay=config.get("weight_decay", 1e-4)
            )

            best_f1 = 0.0
            patience_counter = 0
            patience = 5

            num_epochs = config.get("num_epochs", 20)

            print(f"üöÄ –ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ {num_epochs} –µ–ø–æ—Ö")

            for epoch in range(num_epochs):
                # –†–æ–∑–º–æ—Ä–æ–∂—É—î–º–æ –ø—ñ—Å–ª—è freeze_epochs
                if epoch == freeze_epochs:
                    for param in model.parameters():
                        param.requires_grad = True
                    print("üî• –ú–æ–¥–µ–ª—å —Ä–æ–∑–º–æ—Ä–æ–∂–µ–Ω–∞")

                # –ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è
                train_loss, train_acc = train_epoch_optimized(model, train_loader, criterion, optimizer, device)
                val_loss, val_acc, val_f1 = validate_optimized(model, val_loader, criterion, device)

                # –õ–æ–≥—É–≤–∞–Ω–Ω—è
                wandb.log({
                    "epoch": epoch,
                    "train_loss": train_loss,
                    "train_acc": train_acc,
                    "val_loss": val_loss,
                    "val_acc": val_acc,
                    "val_f1": val_f1,
                })

                print(f"–ï–ø–æ—Ö–∞ {epoch + 1}: Train Acc={train_acc:.3f}, Val F1={val_f1:.3f}")

                # Early stopping
                if val_f1 > best_f1:
                    best_f1 = val_f1
                    patience_counter = 0
                else:
                    patience_counter += 1

                if patience_counter >= patience:
                    print(f"‚èπÔ∏è Early stopping –ø—ñ—Å–ª—è {epoch + 1} –µ–ø–æ—Ö")
                    break

            print(f"üéâ –ù–∞–π–∫—Ä–∞—â–∏–π F1: {best_f1:.3f}")

            # –§—ñ–Ω–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è
            wandb.log({"final_f1": best_f1})

        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
            wandb.log({"val_f1": 0.0})


def main():
    parser = argparse.ArgumentParser(description="W&B Sweeps –¥–ª—è –ø–æ—à—É–∫—É –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤")
    parser.add_argument("--create", action="store_true", help="–°—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–æ–≤–∏–π sweep")
    parser.add_argument("--agent", type=str, help="–ó–∞–ø—É—Å—Ç–∏—Ç–∏ –∞–≥–µ–Ω—Ç –∑ –≤–∫–∞–∑–∞–Ω–∏–º sweep ID")
    parser.add_argument("--count", type=int, default=15, help="–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø—É—Å–∫—ñ–≤ –¥–ª—è –∞–≥–µ–Ω—Ç–∞")

    args = parser.parse_args()

    # –í–ê–ñ–õ–ò–í–û: –Ø–≤–Ω–æ –≤–∫–∞–∑—É—î–º–æ –ø—Ä–æ–µ–∫—Ç –¥–ª—è –≤—Å—ñ—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π
    PROJECT_NAME = "agri-risk-hyperparameter-search"

    if args.create:
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–æ–≥–æ sweep
        sweep_id = wandb.sweep(SWEEP_CONFIG, project=PROJECT_NAME)
        print(f"üÜî –°—Ç–≤–æ—Ä–µ–Ω–æ sweep: {sweep_id}")
        print(f"üåê URL: https://wandb.ai/msas-agrichain/{PROJECT_NAME}/sweeps/{sweep_id}")
        print(f"üöÄ –î–ª—è –∑–∞–ø—É—Å–∫—É –∞–≥–µ–Ω—Ç–∞: python sweep.py --agent {sweep_id}")

    elif args.agent:
        # –ó–∞–ø—É—Å–∫ –∞–≥–µ–Ω—Ç–∞ –∑ –Ø–í–ù–ò–ú –≤–∫–∞–∑–∞–Ω–Ω—è–º –ø—Ä–æ–µ–∫—Ç—É
        print(f"ü§ñ –ó–∞–ø—É—Å–∫ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è sweep {args.agent}")
        print(f"üìä –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤: {args.count}")
        print(f"üìÅ –ü—Ä–æ–µ–∫—Ç: {PROJECT_NAME}")

        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –¥–æ–¥–∞—î–º–æ project –¥–æ wandb.agent
        wandb.agent(
            args.agent,
            function=train_model_sweep,
            count=args.count,
            project=PROJECT_NAME  # ‚úÖ –î–û–î–ê–ù–û!
        )

    else:
        # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º —Å—Ç–≤–æ—Ä—é—î–º–æ sweep —Ç–∞ –∑–∞–ø—É—Å–∫–∞—î–º–æ
        print(f"üÜî –°—Ç–≤–æ—Ä–µ–Ω–Ω—è sweep —É –ø—Ä–æ–µ–∫—Ç—ñ: {PROJECT_NAME}")
        sweep_id = wandb.sweep(SWEEP_CONFIG, project=PROJECT_NAME)
        print(f"üÜî –°—Ç–≤–æ—Ä–µ–Ω–æ sweep: {sweep_id}")
        print(f"üöÄ –ó–∞–ø—É—Å–∫ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤...")

        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –¥–æ–¥–∞—î–º–æ project –¥–æ wandb.agent
        wandb.agent(
            sweep_id,
            function=train_model_sweep,
            count=10,
            project=PROJECT_NAME  # ‚úÖ –î–û–î–ê–ù–û!
        )


if __name__ == "__main__":
    main()