# Practice

***

# H5: Training & Experiments

## Reading list:

- [The Data Science Lifecycle Process](https://github.com/dslp/dslp#the-data-science-lifecycle-process)
- [Structuring Your Project](https://docs.python-guide.org/writing/structure/)
- [How to Organize Deep Learning Projects – Examples of Best Practices](https://neptune.ai/blog/how-to-organize-deep-learning-projects-best-practices)
- [Readme Driven Development](https://tom.preston-werner.com/2010/08/23/readme-driven-development.html)
- [Reengineering Facebook AI’s deep learning platforms for interoperability](https://ai.meta.com/blog/reengineering-facebook-ais-deep-learning-platforms-for-interoperability/)
- [The Twelve Factors](https://12factor.net/)
- [Weights & Biases Documentation](https://docs.wandb.ai/)
- [The value of a shared understanding of AI models](https://modelcards.withgoogle.com/about)
- [Aim experiment tracker](https://github.com/aimhubio/aim)
- [MosaicBERT: Pretraining BERT from Scratch for $20](https://www.mosaicml.com/blog/mosaicbert)
- [AutoML NNI](https://github.com/microsoft/nni)
- [15 Best Tools for ML Experiment Tracking and Management](https://neptune.ai/blog/best-ml-experiment-tracking-tools)
- [Some Techniques To Make Your PyTorch Models Train (Much) Faster](https://sebastianraschka.com/blog/2023/pytorch-faster.html)
- [Pytorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)
- [Distributed Training](https://www.run.ai/guides/gpu-deep-learning/distributed-training)
- [Training 175B Parameter Language Models at 1000 GPU scale with Alpa and Ray](https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray)
- [Accelerate](https://github.com/huggingface/accelerate)
- [Ray Train: Scalable Model Training](https://docs.ray.io/en/latest/train/train.html#train-docs)

## Task:

You need to have a training pipeline for your model for this homework. You can take it from your test task for this course, bring your own or use this [code](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) as an example.


- Update the Google Doc with the experiment section: experiment management tool and model card for your project.
- PR1: Write code for training your model using the W&B experiment logger.
- PR2: Write code for conducting hyperparameter searches with W&B.
- PR3: Write code to create a model card for your model, which can be a simple markdown or utilize this [toolset](https://github.com/tensorflow/model-card-toolkit)
- PR4 (optional): Write to replicate the [tutorial](https://www.mosaicml.com/blog/mosaicbert)
- PR5 (optional): Write code for hyperparameter searches using [NNI](https://github.com/microsoft/nni)
- PR6 (optional): Write code for distributed training with PyTorch, Accelerate, and Ray.
- Public link to your W&B project with experiments.

## Criteria: