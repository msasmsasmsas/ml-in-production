H10: Inference servers
Reading list:

    Machine learning system design pattern
    Seldon Core v2
    TorchServe
    Triton Inference Server
    Ray Serve
    SageMaker Inference Toolkit
    Overview of getting predictions on Vertex AI
    Qwak Model Serving
    ModalLab Fast inference with vLLM (Mistral 7B)
    Large Language Model Text Generation Inference
    Easy, fast, and cheap LLM serving for everyone
    Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs
    Book: Machine Learning Systems with TinyML

Task:

    PR1: Write code for Seldon API deployment of your model, including tests.
    PR2: Write code for KServe API integration with your model, including tests.
    PR3: Write code for Triton Inference Server deployment, incorporating tests.
    PR4: Write code for Ray deployment, complete with tests.
    PR5 (optional): Write code for LLM deployment using TGI, vLLM, and LoRAX.
    PR6 (optional): Write code for LLM deployment with ModalLab.
    Update the Google document on model serving, outlining options and comparisons between custom servers and inference servers. Decide and explain which solution you will use and why.

Criteria:

    6 PRs merged
    Serving comparisons and conclusion in the google doc.
