# TorchServe configuration

# Inference API port
inference_address=http://0.0.0.0:8080

# Management API port
management_address=http://0.0.0.0:8081

# Metrics API port
metrics_address=http://0.0.0.0:8082

# Model store directory
model_store=/home/model-server/model-store

# Maximum number of workers per model
default_workers_per_model=4

# Enable metrics API
metrics_mode=prometheus

# Enable response logging
enable_envvars_config=true

# Maximum request size (10 MB)
max_request_size=10485760

# Custom handler directories
install_py_dep_per_model=true
